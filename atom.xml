<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://benschmidt.org/</id>
    <title>Blog Title</title>
    <updated>2023-02-10T04:30:31.705Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <author>
        <name>Ben Schmidt</name>
        <email>bmschmidt@gmail.com</email>
        <uri>https://benschmidt.org</uri>
    </author>
    <link rel="alternate" href="https://benschmidt.org/post"/>
    <link rel="self" href="https://benschmidt.org/feed.xml"/>
    <subtitle>Posts and updates. Fun with a porpoise.</subtitle>
    <rights>All rights reserved 2023, Ben Schmidt</rights>
    <category term="History"/>
    <category term="Programming"/>
    <category term="Digital Humanities"/>
    <category term="Data Analysis"/>
    <category term="Data Visualization"/>
    <entry>
        <title type="html"><![CDATA[A nested post.]]></title>
        <id/>
        <link/>
        <updated>2023-02-09T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<p>Do you see how the URL for this post is nested? That’s what this demonstrates. Look in the folder of markdown folders to see that it shows up in a folder. You could do that all you want.</p>
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last Post]]></title>
        <id/>
        <link/>
        <updated>2023-02-02T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<p>This is the most recent post I’ve made! It’s in Markdown and everything.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Pandoc markdown, that is, which is the best Markdown that there is.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Peer review" is younger than you think. Does that mean it can go away?]]></title>
        <id/>
        <link/>
        <updated>2017-09-15T15:41:00.004Z</updated>
        <content type="html"><![CDATA[<p><em>This is a blog post I’ve had sitting around in some form for a few years; I wanted to post it today because:</em></p>
<p><em>1) It’s about peer review, and it’s <a href="https://peerreviewweek.wordpress.com/">peer review week</a>! I just read <a href="https://dukeupress.wordpress.com/2017/09/14/editorial-director-ken-wissoker-on-why-he-loves-peer-review/">this nice piece by Ken Wissoker</a> in its defense.</em><br />
<em>2) There’s a conference on <a href="https://rrchnm.org/news/arguing-with-digital-history-workshop-to-address-a-central-problem-in-digital-history/">argumentation in Digital History</a> this weekend at George Mason which I couldn’t attend for family reasons but wanted to resonate with at a distance. </em></p>
<p><em>It’s still sketchy in places, but I’m putting it up as a provocation to think (and to tell me) more about the history of peer review, and how fundamentally malleable scholarly norms are, rather than as a completed historical essay in its own right. [Edit–for a longer and better-informed version of many of these points, particularly as they relate to the sciences, Konrad Lawson points out <a href="https://www.timeshighereducation.com/features/peer-review-not-old-you-might-think">this essay by Aileen Fyfe</a>; my old grad school colleague Melinda Baldwin has an essay in Physics Today from her forthcoming project that covers <a href="http://physicstoday.scitation.org/doi/10.1063/PT.3.3463">the whole shebang as well, with a particular emphasis on physics</a>.]</em></p>
<p>It’s easy, when writing about “the digital,” to become foolishly besotted by the radical transformation it offers. There’s sometimes a millenarian strand in the digital humanities that can be dangerous, foolish, or both, and which critics of the field occasionally seize on as evidence of its perfidy. But it’s just as great a betrayal of historical thinking to essentialize the recent past as to hope that technology lets us uproot the past. We should not fall short of imagining the changes that are possible in the disciplines; and we shouldn’t think that disciplines need revolve around particular ways of reviewing, arguing, or producing scholarship.</p>
<p>Here’s a short historical story about one thing we tend to essentialize, peer review. I find it useful for illustrating two things. The first is that scholarly concepts we think of as central to the field are often far more recent than we think. This is, I think, a hopeful story; it means the window for change may also be greater than we think. The second is that they are, indeed, intricately tied up with social and technological changes in living memory; the humanities are not some wonderful time container of practices back to Erasmus or even Matthew Arnold. I’m posting it now, after delivering it as a hand-wavy talk at Northeastern in 2015.</p>
<p>Peer review seems to be so fundamental to scholarship that we can hardly imagine a world without it. Conventional histories of peer review suggest that it is old indeed. Kathleen Fitzpatrick starts <a href="http://mcpress.media-commons.org/plannedobsolescence/one/the-history-of-peer-review/">her discussion of its history</a> in the 1750s, although she suggests that the “history of peer review thus appears to have been both longer and shorter than we may realize,” extending back to the 17th century but still imperfect by the 1940s. Wikipedia editors are more firm in their straightforward assertion that is was developed by <a href="https://en.wikipedia.org/wiki/Henry_Oldenburg" title="Henry Oldenburg">Henry Oldenburg</a> (1619–1677), who built on the work of <a href="https://en.wikipedia.org/wiki/Al-Ruhawi" title="Al-Ruhawi">Ishāq ibn ʻAlī al-Ruhāwī</a> (854–931). (Wikipedia is less clear on just how it quietly gestated for 7 centuries.)</p>
<p>But even if peer review is ancient, “peer review” itself is quite new. I was surprised, a few years ago, in performing anachronism consulting for the show “Masters of Sex,” set in the early 1960s, to see my algorithms reject one character’s suggestion that Masters and Johnson needed to publish in peer reviewed journals as hopelessly anachronistic. But that is indeed the case. Google Ngrams shows only sporadic uses before about 1970; the adjectival form “peer reviewed,” as adhering to scholarship, barely exists before 1980. (As always, you should basically ignore Google Ngrams results from after 2000, but why not include them?)</p>
<p><a href="https://3.bp.blogspot.com/-seXThjZaWsY/WXpO-g3ToFI/AAAAAAAAJMw/4Qe3x0Q9CTUyva2N99K6empKAQrz1hy5ACLcBGAs/s1600/Screen%2BShot%2B2017-07-27%2Bat%2B4.00.20%2BPM.png"><img src="https://3.bp.blogspot.com/-seXThjZaWsY/WXpO-g3ToFI/AAAAAAAAJMw/4Qe3x0Q9CTUyva2N99K6empKAQrz1hy5ACLcBGAs/s1600/Screen%2BShot%2B2017-07-27%2Bat%2B4.00.20%2BPM.png" /></a></p>
<p>Of course the thing may exist before the word: but one thing I’ve found invariably in looking at these etymologies is words usually do not march straight out of the primordial ooze into widespread use for no reason at all, particularly words describing so specific and unpoetic as a practice like this. Usually there is some reason, some new thing in the world that requires a new term to distinguish it from what has come before.</p>
<p>So what new thing gave rise to peer review? Reading through the texts gives some sense. JStor contains no uses of the phrase until 1965, in reference to “peer review groups” at the NIH. Through the late 1960s the phrase was only used in the context of doctors supervising medical care of other doctors. The first use outside of medicine I find is in 1969, in a library sciences context,1 also about professional self-evaluation. An early usage for grants is in 1970.2 The first usage of the phrase in the American Historical Review is in 1978, in a decidedly negative evaluation of the “peer review bureaucracies of foundations and government,” “manned by scientists of lesser achievement.”</p>
<p>So what is the new thing being described here? As best as I can tell, the reason for its existence is the rise of the new government funding bureaucracy in the 1960s; the NIH, the NSF, and their smaller cousins like the NEH all needed mechanisms to distribute their new government largess: and peer review was a way to ensure that government money was not handed out by the government, but by experts from the scientific community. (This story, by the way, bears an interesting relationship to the one I wrote earlier this month about <a href="http://sappingattention.blogspot.com/2017/07/what-is-described-as-belonging-to.html">government attributes versus public ones</a>; that sentence would have a different valence if I said that scholars wished to ensure the “public money was not handed out by the public.”) Its use in non-publication situations–peer review boards investigating medical malpractice, for example–is almost entirely about protecting professional organizations from state or other bureaucratic interference. And when there are large-scale discussions of peer review in science–as in a <a href="http://journals.sagepub.com/toc/sthd/10/3">special 1985 edition of “Science, Technology, and Human values,”</a> they frequently take grant-making as the archetypal form, not the refereeing of scholarship.</p>
<p>The central technology that causes this new term to spread to scholarship, then, is the grantmaking state. But this relies on a more prosaic technology, as well. Peer review is the archetypal form of scholarly organization in the age of the xerox machine. Without it, a sheaf of grant proposal could not be easily snapped into a binder, mailed across the country, and then (perhaps) flown back to Washington with its expert reader for discussion.</p>
<p>Digging around a little recently, I see that historian of science Alex Czsisar wrote a short piece for Nature in 2016 (after I gave this as a talk, so not fully incorporated here) where he says this, which is very much along the same lines.</p>
<blockquote>
<p>‘Peer review’ was a term borrowed from the procedures that government agencies used to decide who would receive financial support for scientific and medical research. When ‘referee systems’ turned into ‘peer review’, the process became a mighty public symbol of the claim that these powerful and expensive investigators of the natural world had procedures for regulating themselves and for producing consensus, even though some observers quietly wondered whether scientific referees were up to this grand calling.</p>
</blockquote>
<p>All of this suggests, though it doesn’t prove, that the shift to a language of “peer review” involves a model of research that draws on a nationally organized scientific funding system that merges with a series of older traditions. Most of the histories of peer review in the sciences note how late journals were to adopt it: leading British publications like the Lancet and Nature don’t take up outside peer reviewers until the 1970s.</p>
<p>If the history of peer review in the sciences is young, the history of peer review in the humanities is even younger. The earliest usage in the front-or-back matter of the American Historical Review that I see at first glance is from 1996, in describing who can perform book reviews; the second use outside a history-of-science context is in 1997, when Sheila Fitzpatrick notes unhappily that a book has obviously “undergone extensive peer review” in a way that weakens it, serving as an “uncomfortable reminder that peer review may function not only as a gate-keeping procedure but also as a kind of censorship of unpopular opinions.” (Here’s <a href="https://www.ncbi.nlm.nih.gov/pubmed/23054375">a similar 2012 argument</a> in favor of editorial review, *not* peer review, in a science journal). (Not far down the list is Ayers and Thomas’s 2003 digital article “The Differences Slavery Made;” rather than showing up to challenge decades of consensus on peer review, digital scholarship was already arriving on the hardly after the consensus had set).</p>
<p>I’d have to do more research to really understand what the outside review policies of leading journals were in the (say) 1950s and 1960s. I have the impression, but have lost the reference, that at least single-copy referee reviewing was common, if not mandatory, in the period. (“Outside referee,” though, is another 1960s neologism.) Still, the technology of mimeographs and carbon copies makes for different forms of outside review: In 1956, the <a href="https://www.jstor.org/stable/1848581?Search=yes&amp;resultItemClick=true&amp;searchText=%22carbon%20copy%22&amp;searchUri=%2Faction%2FdoBasicSearch%3Ffilter%3Djid%253A10.2307%252Fj100010%26amp%3BQuery%3D%2522carbon%2Bcopy%2522&amp;refreqid=search%3A4774cf3ea701092e473e6c5d6bac3339&amp;seq=2#page_scan_tab_contents">AHR only demanded a single paper copy of each article</a> submitted. (The ribbon copy, please; keep the carbon copy for yourself.) This <a href="https://www.jstor.org/stable/1848020?seq=4#page_scan_tab_contents">seems to have been still true in 1970</a>; I suppose by then the AHR could have been paying for photocopies on their own, but I doubt they did. By 1980, the number of required copies number has increased to 2; by the late 1990s, the frontmatter demanded four, or a Microsoft-compatible disk, which is what it would take for a really modern peer review system. But surely I’ve said enough now that someone will chime in in the comments about how mimeographs were sent to various reviewers in serial.</p>
<p>I know even less about monographs; but it should be widely known that some important work in the field *continues* to be published as part of edited volumes, trade presses, and other channels subject only to editorial review, not peer review.</p>
<p>So what? Obviously peer review didn’t *really* come into being in the historical profession in 1995. That’s ridiculous! Books and journals had various forms of outside referees in the mimeograph/carbon copy age as well. But the point is this: something did change, and more recently than we might think. Peer review was created, rather than always existing in the humanities; its rhetorical adoption in those fields as a core term may be less connected to eternal ideas of scholarship, and have more to do with the effort to make them more like the sciences in the postwar university. Be skeptical of the administrative-ese, buzzword-inflected vogue for the digital humanities all you want: but also, be aware that if you say “peer review is fundamental to sound scholarship,” you’re speaking in the buzzwords of not long ago, yourself.</p>
<p>I wouldn’t argue in good faith that peer review is new, so it’s bad. It might be that the scholarly practices from 1970 to 2010 are indeed worth preserving.</p>
<p>But at the same time, I’m not sure that digital scholarship could ever fully reconcile itself to peer review in the traditional sense without transforming it enough that we’ll need a whole new phrase to describe the new regime to come.</p>
<p>Randomly selected bibliography:</p>
<p>Some early citations, and some previous histories.</p>
<p>“Atlantic City Conference: A Great Show–in Two Parts and a Cast of Thousands.” ALA Bulletin 63, no. 7 (): 915–964. http://www.jstor.org/stable/ 25698237.</p>
<p>Cooper, Joseph D. “Onward the Management of Science: The Wooldridge Report.” Science 148, no. 3676. New Series (): 1433–1439. http://www.jstor.org/ stable/1716537.</p>
<h2 id="csiszar-alex.-peer-review-troubled-from-the-start.-nature-news-532-no.-7599-april-21-2016-306.-doi10.1038532306a.">Csiszar, Alex. “Peer Review: Troubled from the Start.” Nature News 532, no. 7599 (April 21, 2016): 306. doi:10.1038/532306a.</h2>
<h3 id="comments">Comments:</h3>
<h4 id="very-important-work-worth-continuing-and-spreadin">Very important work, worth continuing and spreadin…</h4>
<p><a href="https://www.blogger.com/profile/16865985571583978032" title="noreply@blogger.com">Brett</a> - <time datetime="2017-09-16T22:04:34.603-04:00">Sep 6, 2017</time></p>
<p>Very important work, worth continuing and spreading far and wide. Well done.</p>
<hr />
<h4 id="as-always-you-should-basically-ignore-googl">“As always, you should basically ignore Googl…</h4>
<p><a href="https://www.blogger.com/profile/11920109042402850214">Steve Sailer</a> - <time datetime="2017-09-17T01:44:07.138-04:00">Sep 0, 2017</time></p>
<p>“As always, you should basically ignore Google Ngrams results from after 2000, but why not include them?”</p>
<p>My impression is that 2008 (the last year in the data base) has only partial data, which is why if you set the time period to run through 2008 the graph almost always dips precipitously at the end. Because nGram normally smooths results, this makes all the recent data look bad.</p>
<p>However, I think the data is pretty good through 2007.</p>
<hr />
<h4 id="growth-parallel-to-emerging-internet">Growth parallel to emerging Internet?</h4>
<p><a href="https://www.blogger.com/profile/07410690681268905487">Antony</a> - <time datetime="2017-09-17T23:09:36.944-04:00">Sep 0, 2017</time></p>
<p>Growth parallel to emerging Internet?</p>
<hr />
<h4 id="for-the-adoption-of-peer-review-in-the-humanities">For the adoption of peer review in the humanities …</h4>
<p><a href="https://www.blogger.com/profile/15866706898579276306">Aileen Fyfe</a> - <time datetime="2017-09-18T16:07:13.529-04:00">Sep 1, 2017</time></p>
<p>For the adoption of peer review in the humanities and social sciences in the 1950s/60s (and particularly for the choice of double-blind rather than single blind) see Pontille and Torny:<br />
http://adanewmedia.org/2014/04/issue4-pontilletorny/</p>
<hr />
<h4 id="youre-entirely-right-that-the-enthusiasm-for">You’re entirely right that the enthusiasm for …</h4>
<p><a href="https://www.blogger.com/profile/15866706898579276306">Aileen Fyfe</a> - <time datetime="2017-09-20T04:07:47.488-04:00">Sep 3, 2017</time></p>
<p>You’re entirely right that the enthusiasm for the term ‘peer review’ is new, as the forthcoming papers from Melinda Baldwin, and from me (with Noah Moxham) make clear for the sciences. Yes, it seems to be linked to the rise of the grant-making state. But there’s also an interesting geographical question: all the instances we know about so far (see Baldwin) are about the USA. So why/when did other countries jump on the peer review bandwagon? We know very little about that…</p>
<p>Re technology: I agree that editorial requests to authors to send in multiple copies does suggest that a reviewing process is going on. However, the lack of such request does not imply the opposite. Throughout the 19thC and early 20thC, the Royal Society (London) sent the single unique copy of the manuscript to one, two or more referees as necessary, one after the other; and thankfully the Victorian postal service was pretty good (the only issues I’ve spotted in the archive about papers getting lost involved them being mislaid in the referee’s house, not lost in the post).</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The efficient plots hypothesis]]></title>
        <id/>
        <link/>
        <updated>2016-09-09T17:07:00.002Z</updated>
        <content type="html"><![CDATA[<p>I’m pulling this discussion out of the comments thread on <a href="http://www.lagado.name/blog/brownian-noise-and-plot-arcs/">Scott Enderle’s blog</a>, because it’s fun. This is the formal statement of what will forever be known as the <strong>efficient plot hypothesis for plot arceology</strong>. Noble prize in culturomics, here I come.</p>
<p>Brief background: Enderle shows pretty persuasively that all the fundamental plot arcs <a href="https://arxiv.org/abs/1606.07772">described in a paper by a math-based computational story lab</a> can be ascribed to random (brownian) noise. As I wrote <a href="https://sappingattention.blogspot.com/2016/07/plot-arceology-emotion-and-tension.html">earlier</a>, and <a href="https://litlab.stanford.edu/humanities-without-humanists/">Hannah Walser explored in more depth recently</a>, that this happens with their data isn’t so surprising; the “stories” they are modeling are mostly random documents to begin with.</p>
<p>Still, there’s some reason to think that maybe sentiment trajectories are random walks even in actual databases of stories like those Matt Jockers uses. Enderle finds that, well, weird: “Should we find that sentiment data from novels does indeed amount to “mere noise,” literary critics will have some very difficult questions to ask themselves about the conditions under which noise signifies.” The idea that plots are random seems offensive to the idea of plot at all. Others in the field, like Jockers and Ted Underwood, have also expressed the idea that there should be some regularities to plot, particularly that map across genre.</p>
<p>I had earlier raised the idea that <a href="https://sappingattention.blogspot.com/2016/07/plot-arceology-emotion-and-tension.html">the null hypothesis for plot testing should be a random walk</a> (Brownian noise, as Enderle calls it) but I thought of it as just that–a null hypothesis that indicates nothing interesting is going on.</p>
<p>But of course, it *would be interesting if nothing was going on.* It would demand explanation! And now I’ve got one: the <strong>efficient plots hypothesis,</strong> a corollary of the efficient markets hypothesis (EMH) for the literary world.</p>
<p>The EMH states that stock prices are efficient; you can’t know reliably if they’re about to go up or down, because if they were someone would have bought them. There’s been a lot of research on whether stocks move in Brownian noise; they don’t, totally, but they come pretty close.<br />
The EPH, as I imagine it, says that the ideal reader can’t know if the mood of a book is about to get sunnier or darker at any given point in the plot. This not because of market forces directly, but because the purpose of a narrative is to engross the reader. Engrossment proceeds through uncertainty. If you knew what was about to happen, you’d skim ahead or stop reading.</p>
<p>That is: at any moment in a story, the emotional trajectory is a random walk for the reader because anything else would be *boring.* And stories aren’t boring.</p>
<p>This could be tested empirically by asking readers if a book will get more positive or more negative over the next five pages, and by how much. In a pure EPH world, they’ll only be right about half the time. Enderle thinks the EPH is obviously wrong, particularly for genre fiction.</p>
<p>I’m not so sure. To take an example: I read some John le Carré novels over the summer. Periodically, a spy has to secretly pass from the East to the West without getting by the commies. (Through the Berlin wall, over the Chinese border to Hong Kong, etc.) Do you know if they’ll make it? The emotional sentiment of the next few pages depends on whether they get killed or not. I can see two models here:<br />
1. Genre determines plot arceology: There are conventions to the spy novel that make it possible to tell in advance.<br />
2. The EPH: The whole point of reading a spy novel is that you <em>don’t</em> know what will happen; the job of a spy novelist is to make you unsure.</p>
<p>My reading experience is much closer to the latter; that the conventions of genre fiction are *precisely* that you don’t know what’s going to happen next; otherwise no one would read it.<br />
For most good genre fiction, I think this holds. Will Lockhardt/Gardner win the case? Is Don Draper going to hit the bottle or stay sober? The rise of “<a href="http://variety.com/2016/tv/opinion/tv-deaths-walking-dead-the-100-arrow-1201751968/">anyone can die</a>” as the predominant trope of 2010s TV suggests that the economics are forcing stronger and stronger forms of the EPH onto us every day.</p>
<p>The major objection to this would be: “but there *are* genres where you know the outcomes precisely!” In a Hardy Boys novel, they’ll rebound from danger and catch the bad guy every time. One response to this is: sure, *you* know that; but you don’t read Hardy boys novels. The people who do are 10-year-olds who legitimately think that, just maybe, the killer’s going to drown the brothers in the quarry and the next 20 books on the shelf will turn out to be prequels.</p>
<p>Even if you know how certain books will *end*, that doesn’t mean that you’ll ever be able to predict the <strong>next two pages,</strong> which is what this is about. I think this distinction is crucially important and maybe underestimated. Sure, a romantic comedy always has a temporary breakup in the middle; but whether that happens 40% of the way through or 70% of the way through makes all the difference; and if you’ve made it 90% of the way through without the breakup happening, you start to think “maybe this is one of those comedies without a breakup in it.”</p>
<p>If the EPH holds, then, it doesn’t suggest that fiction is truly arbitrary; rather, that it’s an elaborately constructed game between reader and writer, socially conditioned and in no way permanent. It would suggest that there are enough fundamental plots that at any point in a book you are unsure what plot you are in; and that plots tend to wear themselves out over time.</p>
<p>It does completely throw into the ringer my analogy between musical tonality and emotional valence. Key signatures in music are highly predictable. But I think that’s OK: it’s really clear that there aren’t underlying structures quite so strong as sonata form under novels; this would explain why.</p>
<p>For a lunatic idea, the EPH is actually empirically kind of testable. Just ask people to predict the direction of books as they’re reading them. Someone could totally do this. Maybe some movie studios even do.</p>
<h2 id="for-more-details-see-my-forthcoming-book-with-stephen-dubner-jane-austen-was-a-derivatives-trader-harper-collins-2017.">For more details, see my forthcoming book with Stephen Dubner, <em>Jane Austen was a Derivatives Trader </em> (Harper Collins 2017)<em>.</em></h2>
<h3 id="comments">Comments:</h3>
<h4 id="this-was-really-interesting-to-read-as-a-fiction-w">This was really interesting to read as a fiction w…</h4>
<p><a href="https://www.blogger.com/profile/07420770741938876807" title="noreply@blogger.com">kb</a> - <time datetime="2016-09-11T11:17:41.969-04:00">Sep 0, 2016</time></p>
<p>This was really interesting to read as a fiction writer. I wonder if the tension between knowing how a book will end and not knowing what will be in the next two pages is actually crucial to some extent.</p>
<p>I recently finished my MFA and one technique that the writers there sometimes mention is the idea of pulling readers through a scene as opposed to pushing them through a scene. Basically, you say at the start of the story or scene how it ends or what the significance of it will be before going into the story in chronological order. The idea is if you say “That morning I sat down at my computer and started reading my emails” the reader isn’t going to be as interested in that sentence as if you say “Let me tell you about the day I met Pablo Escobar. That morning I say down at my computer and started reading my emails”. You set an endpoint but interesting enough to hook the reader and vague enough that the question of how to reach that endpoint is intriguing.</p>
<p>Uncertainty is crucial to an interesting plot, but the reader also wants to trust the author knows what they’re doing and where they’re going. If there’s too much uncertainty, it might undermine the reader’s trust. Having some degree of signposting along the way probably helps.</p>
<hr />
<h4 id="as-a-fiction-writer-i-find-this-way-of-looking-at">As a fiction writer, I find this way of looking at…</h4>
<p><a href="https://www.blogger.com/profile/02088100982761595050">Jim H.</a> - <time datetime="2016-09-11T15:05:58.683-04:00">Sep 0, 2016</time></p>
<p>As a fiction writer, I find this way of looking at fictional texts fascinating and love following the discussions going back and forth with regard to analyses of this art form.</p>
<p>As a philosopher, I have a quick critique of the point of this post. There are two things at work here, and they seem to get conflated in your approach.</p>
<p>First thing: readers of fiction read fiction along a timeline normally beginning from the time they immerse in page one and lasting until they close the book after reading “The End.” They are, let us say, “in time”.</p>
<p>Second thing: textual analysis (such as Jockers’s—with whom, full disclosure, I have been in contact and who has performed a Sentiment Analysis on my first novel’s manuscript after I self-taught R and attempted same myself. But I disgress.) takes a look at a completed text in an, let us call it, arcetextural manner. The novel is complete. It has ended, and the analyst is no longer “in time” with the text. Rather, she is breaking down its linguistic or sentimental, etc., components and comparing them to others.</p>
<p>Here’s an analogy: You are walking along a new road. You don’t know what lies around the next bend—whether there’s a hill or a river you will have to deal with. But I’m flying a drone or satellite above you and can see the length of road you are traversing and know what you’re about to encounter. What’s more, I’ve seen dozens of others taking the same route and witnessed how they dealt with these so-called unknowns.</p>
<p>So, yes, the RWH or EPH works for the individual reader the first time through the text. But on second read and for analytic purposes, the plot is less important. The suspense is removed, and the elements that go into the text’s work as an artwork (or genre work) can be seen for what they are. We notice, for example, bits of foreshadowing. We find parallels in subplots that give things away. We observe characters’ behaviors that show us how they tend to act and witness how they conform (or don’t) in the crucial moment.</p>
<hr />
<h4 id="rereading-is-definitely-a-problem-for-this-theory">Rereading is definitely a problem for this theory;…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-12T07:10:56.579-04:00">Sep 1, 2016</time></p>
<p>Rereading is definitely a problem for this theory; the implication is that no one will re-read a book until they’ve forgotten most of the plot points. That seems pretty unlikely.</p>
<p>It’s certainly true that something like Jockers’s technique takes a high-level view and compares them to others. But this is an answer to what they’re finding. It’s somewhat like mapping roads; but what’s striking so far is that the structures that have been uncovered are not what most people were expecting, at least on the page to page level. They look less like roads that all go to Rome, and more like stock prices that fluctuate randomly.</p>
<p>I don’t think that plots are actually random; they’re carefully constructed. (Like you say). But if they’re carefully constructed to look random–so as to maintain reader interest–that would explain why the <em>aggregate</em> plots Jockers and others have been looking for show a less strong signal than some anticipated.</p>
<hr />
<h4 id="this-is-also-a-problem-for-the-eph-yeah.-because">This is also a problem for the EPH, yeah. Because …</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-12T07:21:36.999-04:00">Sep 1, 2016</time></p>
<p>This is also a problem for the EPH, yeah. Because if you say “Let me tell you about the day I met Pablo Escobar” and then start talking about e-mails, it’s pretty obvious that things are going to get emotionally negative. I can think of two answers, none of which I find especially convincing:</p>
<p>1. A sufficiently advanced sentiment analysis will tag “Pablo Escobar” as negative, and so the sentence itself knocks the sentiment location down; the question is now just if you have a positive or negative encounter with him, relative to the encounters with Pablo Escobar people are likely to have.</p>
<p>2. Although you know that the meeting with Escobar is coming, you don’t know *when*. So although I can predict that “eventually something bad will happen,” that’s a statement so vague that it can apply to anything. And just because you say you’ll meet Pablo Escobar, there’s nothing to say that you (the author) won’t go all Tristram Shandy and just never quite get to the point in the day when Escobar shows up, or that your main character is going to be a twenty-five year old comic character whose parents named him after a drug lord.</p>
<hr />
<h4 id="so-how-many-basic-plots-are-there-two-six-thir">So, how many basic plots are there? Two? Six? Thir…</h4>
<p><a href="https://www.blogger.com/profile/02088100982761595050">Jim H.</a> - <time datetime="2016-09-12T11:12:03.629-04:00">Sep 1, 2016</time></p>
<p>So, how many basic plots are there? Two? Six? Thirteen? Twenty? 25? … As many as there are works of fiction? Folk have been asking and trying to answer this question since at least Aristotle’s time. It’s a fun game, but I don’t believe it can ever really be more than that. It’s never a hard science though data aggregators can, indeed, show us some relevant &amp; interesting trends &amp; tendencies.</p>
<p>A key assumption, here, and one I raised with MJ is that equating sentiment analysis with plot. He notes a correlation. He must. I find difficulty making that link and raised the question of dramatic irony. For example, in my ms. I used one sentence (the last) to throw the entire denouement (the protagonist’s hard-won epiphany and decision to make positive changes to his life) of my novel into question. It’s valence cannot be equated with everything it seeks to undermine, and the sentiment analysis is ill-equipped to bring the entire upward sweeping arc down. It treats it just like every other sentence with a similar number of words that rate analysis.</p>
<hr />
<h4 id="i-agree-with-most-of-what-you-say-here.-although">I agree with most of what you say here. (Although …</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-12T11:39:41.055-04:00">Sep 1, 2016</time></p>
<p>I agree with most of what you say here. (Although most of human knowledge is somewhere in between ‘hard science’ and a ‘fun game’.) <a href="https://sappingattention.blogspot.com/2016/07/plot-arceology-emotion-and-tension.html">I’v raised similar questions about whether sentiment analysis is the right tool here;</a> in my attempts at this <a href="http://dl.acm.org/citation.cfm?id=2878250">I’ve used topic models to ask what the closest thing to a single overarching plot arc in a corpus,</a> which is not a method that lends itself well to declaring any particular number of basic clusters.</p>
<p>I’m also very sympathetic to the idea that the last sentence of a book potentially bears much more weight than any other; I think treating all sentences as equal deeply bears little relation to how readers read plot. I don’t know if there’s a good mathematical way to account for this.</p>
<p>If I can toss out yet another oddball thought in the comments, because it’s not worthy of living anywhere else: My gut instinct about distributions is that there are infinitely many possible plots, but that some are much more common. Distributionally I’d bet that usage follows Zipf’s law; so the number of possible plots is proportional to the logarithm of all the books ever written, and the most common single ur-plot (which I’d bet is the marriage plot, in some form, or else something like Campbell’s hero’s journey, but who knows…) is something like 2-10% of all the stories ever told. I am suspicious of methods that too eagerly lump oddball plots into larger categories, which I think the sentiment ones probably do.</p>
<hr />
<h4 id="i-love-your-term-ur-plot.-at-my-blog-i-used-my-no">I love your term ur-plot. At my blog, I used my no…</h4>
<p><a href="https://www.blogger.com/profile/02088100982761595050">Jim H.</a> - <time datetime="2016-09-12T16:18:33.208-04:00">Sep 1, 2016</time></p>
<p>I love your term ur-plot. At my blog, I used my notion of Ur-Story—which I derived from The Epic of Gilgamesh—as the analytical framework to examine literary texts from ancients to moderns (Ivan Ilych, Hamlet, The Erasers [Robbe-Grillet], The Third Policeman, Pnin, Remainder, The Hour of the Star, The Metamorphosis, Malone Dies, Things Fall Apart, Henderson the Rain King, The Loser [Bernhard] and others). This was less a systematic academic exercise than a writer’s reading—in order to inform my own work.</p>
<p>My own opinion is less the marriage plot and more Campbell’s (who also derived much of his view from Gilgamesh &amp; Carl Jung). Briefly, (great) literature (fiction, poetry, tragedy, comedy) arises as a direct response to becoming conscious of the scandal of mortality—the brute fact of realizing we are all of us going to die. The forms are myriad—laughing at the absurdity of it, it’s unavoidably tragic, let’s investigate and find someone to blame, surely there’s a way out, but love is eternal isn’t it?, make hay &amp; entertain me while the sun shines, fuck it all, and on and on. (Even myth and religion grow out of this awareness, proposing consolation and a possible solution however misguided—and much of it makes for great literature.) It was Gilgamesh’s great grief at the death of his boon wildman companion that set him off on his hero’s journey to inquire of Utnapishtim the secret of immortality. That’s the nutshell.</p>
<p>I would wager greater than 2-10% of great literature finds its motivation in this Ur-Story—possibly ALL great literature does; at least that’s my feeling—my opinion. But great literature probably constitutes somewhat less than 2% of all stories ever told.</p>
<p>And, of course, crucial to this Comment is the distinction between Story (which I take to be the Substance of literature) and Plot (which I take to be the Form).</p>
<hr />
<h4 id="great-conjecture.-its-driving-me-crazy-that-i">Great conjecture. It’s driving me crazy that I…</h4>
<p><a href="https://www.blogger.com/profile/04012428899328561750">Ted Underwood</a> - <time datetime="2016-09-13T16:26:35.312-04:00">Sep 2, 2016</time></p>
<p>Great conjecture. It’s driving me crazy that I don’t have time to test it. Totally doable.</p>
<p>For what it’s worth, my money is against the EPH. But the testable hypothesis is the thing!</p>
<hr />
<h4 id="even-if-you-know-how-certain-books-will">1) “Even if you know how certain books will *…</h4>
<p><a href="https://www.blogger.com/profile/08360044945265178991">Bill Benzon</a> - <time datetime="2016-09-13T19:10:35.529-04:00">Sep 2, 2016</time></p>
<ol type="1">
<li>“Even if you know how certain books will *end*, that doesn’t mean that you’ll ever be able to predict the next two pages, which is what this is about. I think this distinction is crucially important and maybe underestimated.”</li>
</ol>
<p>YES.</p>
<ol start="2" type="1">
<li><p>This kind of thing has come up in relation to music. As you know Leonard Meyer postulated expectation and surprise as the driving force behind emotion in music back in 1956. However, the “Surprise Symphony” will surprise you only once, or twice if you’re a bit dense. How is it that music is pleasurable even once we know a piece quite well? One recent answer is that, while there’s one mental module that does indeed know the musical future, there’s one or more others that do not. And it’s those modules that are being fooled. Alas, I don’t have a citation to this idea.</p></li>
<li><p>Finally, this seems obliquely relevant:</p></li>
</ol>
<p>Hays, D. G. (1973). “Language and Interpersonal Relationships.” <em>Daedalus</em> 102(3): 203-216.</p>
<p>pp. 204-205:</p>
<p>The experiment strips conversation down to its barest essentials by depriving the subject of all language except for two pushbuttons and two lights, and by suggesting to him that he is attempting to reach an accord with a mere machine. We brought two students into our building through different doors and led them separately to adjoining rooms. We told each that he was working with a machine, and showed him lights and pushbuttons. Over and over again, at a signal, he would press one or the other of the two buttons, and then one of two lights would come on. If the light that appeared corresponded to the button he pressed, he was right; otherwise, wrong. The students faced identical displays, but their feedback was reversed: if student A pressed the red button, then a moment later student B would see the red light go on, and if student B pressed the red button, then student A would see the red light. On any trial, therefore, if the two students pressed matching buttons they would both be correct, and if they chose opposite buttons they would both be wrong.</p>
<p>We used a few pairs of RAND mathematicians; but they would quickly settle on one color, say red, and choose it every time. Always correct, they soon grew bored. The students began with difficulty, but after enough experience they would generally hit on something. . . . The students, although they were sometimes wrong, were rarely bored. They were busy figuring out the complex patterns of the machine.</p>
<p>But where did the patterns come from? Although neither student knew it, they arose out of the interaction of two students.</p>
<hr />
<h4 id="i-love-this-additional-turn-of-scott-es-null">I love this additional turn of Scott E’s null …</h4>
<p><a href="https://www.blogger.com/profile/00283384044749167124">Unknown</a> - <time datetime="2016-09-16T11:38:10.796-04:00">Sep 5, 2016</time></p>
<p>I love this additional turn of Scott E’s null hypothesis. LeCarre makes a problematic example of the genre writer, though. In some work Scott and I have been doing we find the novels of LeC to be extreme outliers in a corpus of bestsellers; an algorithm that reliably distinguishes genre bestsellers from prizewinning literary fiction gets all the LeC novels wrong. – Jim</p>
<hr />
<h4 id="section"></h4>
<p><a href="https://www.blogger.com/profile/00283384044749167124">Unknown</a> - <time datetime="2016-09-16T11:38:43.221-04:00">Sep 5, 2016</time></p>
<p>This comment has been removed by a blog administrator.</p>
<hr />
<h4 id="there-are-a-few-other-intermediate-tests-someone-s">There are a few other intermediate tests someone s…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-16T16:47:21.663-04:00">Sep 5, 2016</time></p>
<p>There are a few other intermediate tests someone should run here too.</p>
<p>1. Do the patterns look the same if you take random subsets of stories as if you take full stories? (If so, stories are either random noise or some sort of fractal structure; the latter would be weirdly interesting too.)</p>
<p>2. How accurately can a machine learning algorithm predict the next sentiment in a story? What information (ie, what time horizon of previous sentiments) makes it the best at predicting?</p>
<p>These two don’t even need experiments.</p>
<hr />
<h4 id="the-music-analogy-remains-really-interesting.-of-c">The music analogy remains really interesting. Of c…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-16T16:53:22.449-04:00">Sep 5, 2016</time></p>
<p>The music analogy remains really interesting. Of course in music you do know most of what’s going to happen, but some individual part is usually elusive. Maybe.</p>
<p>That brings up yet another similar experiment, Claude Shannon’s experiments to predict the next letter in English. He, of course, found heavy redundancy in English-language text, not a random walk. This question about emotional valence is several levels out from the resolution Shannon was looking at, and I can’t quite wrap my head around the implications. But one possibility is that even some predictability of emotions in the short term (contrary my comment to Ted above) doesn’t necessarily imply that the long term isn’t at least somewhat pseudo-random.</p>
<hr />
<h4 id="concerning-music-im-a-jazz-musician-and-a-p">Concerning music, I’m a jazz musician, and a p…</h4>
<p><a href="https://www.blogger.com/profile/08360044945265178991">Bill Benzon</a> - <time datetime="2016-09-16T23:31:14.814-04:00">Sep 5, 2016</time></p>
<p>Concerning music, I’m a jazz musician, and a pretty good one, too, having opened for Dizzy Gillespie on one occasion and BB King on another. That is to say, my experience is not that of a semi-competent dabbler.</p>
<p>When I’m soloing, then, I am just making it up, but I don’t necessarily know what’s coming next despite the fact that I’m the one who determines it. Sometimes it just gets away from you; what comes out isn’t what you’d been intending. That’s when things get really interesting. Now you’ve got to get back on track so you can pretend that the ‘mistake’ wasn’t a mistake at all, just a particularly clever feint. Hence the saying, “there are no bad notes, only bad resolutions.”</p>
<p>Note however that if you listen to the older guys (Louis Armstrong for example) play the same tune on different recordings, you realize that they aren’t making their solo up from scratch each time. Rather, on a given tune they’ve got an approach worked out and they follow it each time they play the tune, with only minor variations. It’s not improvisation so much as it is un-notated composition. The idea of improvising a new solo each and every time seems to have evolved with the bebop era in the early to mid-1940s.</p>
<hr />
<h4 id="interesting.-i-assume-that-youre-classifying">Interesting. I assume that you’re classifying …</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-20T10:16:37.510-04:00">Sep 2, 2016</time></p>
<p>Interesting. I assume that you’re classifying on vocabulary, so it’s *possible* that LeCarre books have a literary prizewinner’s diction but a thriller’s structure. (That sounds like back-jacket copy, doesn’t it?). But for me to just assert that is kind of begging the question here.</p>
<p>I guess another way of saying it is: do plots become more predictable in genres where they’re of primary importance? I haven’t really read enough true pulps to say.</p>
<hr />
<h4 id="dont-tell-me-the-book-my-guess-for-the-story">Don’t tell me the book: my guess for the story…</h4>
<p><a href="https://www.blogger.com/profile/07956598848315576017">Unknown</a> - <time datetime="2016-09-28T19:15:28.158-04:00">Sep 3, 2016</time></p>
<p>Don’t tell me the book: my guess for the story becoming more positive or negative in the next two pages (or five) is ‘neither’. If the goal is ‘what happens’ as you discuss later, my guess is “same thing that is happening now.”</p>
<p>However, I’m not convinced though that an “efficient plot” is meaningful in the way you describe it. What would a perfect EPH book look like? If the criteria is that you can’t predict what will happen in two or five pages, then that’s a book that always zigs when others zag. That’s exhausting and meandering, and I wouldn’t expect that authors optimize for that in good fiction in any way comparable to how the stock market optimizes.</p>
<p>I also have a few clarifying questions.</p>
<p>How do we confirm the EPH? Does it assume the ideal reader or a 10-year old reader? Where do algorithms factor in? If an algorithm can predict the next two pages but a person can’t, does the EPH hold or not?</p>
<p>What about probabilistic predictions? How much certainty do you have to assign to your guess? If a person knows when to not be confident in their guess, does that speak to the efficiency or predictability of the plot?</p>
<hr />
<h4 id="my-guess-for-the-story-becoming-more-positive">&gt; my guess for the story becoming more positive…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2016-09-28T22:10:34.906-04:00">Sep 3, 2016</time></p>
<p>&gt; my guess for the story becoming more positive or negative in the next two pages (or five) is ‘neither’.</p>
<p>Sure–but that’s the a guess consistent with the plot being Brownian noise. The intuition literary scholars seem to have about plot direction is that it’s determined by genre, and you could beat Brownian noise on the margin by knowing your plot.</p>
<p>&gt; What would a perfect EPH book look like? If the criteria is that you can’t predict what will happen in two or five pages, then that’s a book that always zigs when others zag.</p>
<p>If you take the EPH seriously (which, yes, is difficult to do with a straight face) there is no such thing as a single EPH book; it’s a general equilibrium where, from any given point, half of all plots zig and half zag. Neither of those moves is necessarily exhausting. It’s just that half the time the butler did it, and half the time someone else did it.</p>
<p>&gt; How do we confirm the EPH? Does it assume the ideal reader or a 10-year old reader? Where do algorithms factor in? If an algorithm can predict the next two pages but a person can’t, does the EPH hold or not?</p>
<p>As a good hypothesis, I don’t think it could be “proven”; but it should be easy to falsify across any given metric. (I hedge on metric because although the original domain is “sentiment,” I don’t really believe that’s a super proxy for “plot.”) To falsify in any sense, show that plot arcs on that metric *are* predictable in the short term. This should be easy; but the nutty thing that Enderle is finding is that it isn’t as easy as it seems. (I suspect, though, that Matt Jockers has some data sitting around that could kill this thing dead. It may even be in the new book, which I haven’t read yet.)</p>
<p>To extinguish the EPH as a live concern, you could also come up with a more plausible causal explanation than the EPH of why plots aren’t as predictable as you’d think.</p>
<p>The reader it assumes is definitely a weak point. “The intended audience of the book,” maybe?In theory, an algorithm might be able to exceed human performance. But since this is the sort of task that semantic tasks are pretty bad at, I’d probably take it. Probabilistic prediction is fine; in fact, I suspect the best way to score this would be using logits.</p>
<hr />
<h4 id="ah-yes-i-removed-it-earlier-because-i-was-getting">Ah yes, I removed it earlier because I was getting…</h4>
<p><a href="https://www.blogger.com/profile/07956598848315576017">Unknown</a> - <time datetime="2016-09-29T12:44:43.539-04:00">Sep 4, 2016</time></p>
<p>Ah yes, I removed it earlier because I was getting long-winded, but my read was that you intended for half zigs and half zags: where you don’t know if the typical or atypical will happen. I meant that the stated criteria might not necessarily reflect that intent, and might favour a path of all left-turns. Maybe it does, hard to unkink the various possible interpretations in my head.</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Biblio bizarre: who publishes in Google Books]]></title>
        <id/>
        <link/>
        <updated>2014-04-03T17:42:00.002Z</updated>
        <content type="html"><![CDATA[<p>Here’s a little irony I’ve been meaning to post. Large scale book digitization makes tools like Ngrams possible; but it also makes tools like Ngrams obsolete for the future. It changes what a “book” is in ways that makes the selection criteria for Ngrams—if it made it into print, it must have _some _significance—completely meaningless.</p>
<p>So as interesting as Google Ngrams is for all sorts of purposes, it seems it might always end right in 2008. (I could have sworn the 2012 update included through 2011 in some collections; but all seem to end in 2008 now.)</p>
<p>Lo: the Ngram chart of three major publishers, showing the percentage of times each is mentioned compared to all other words in the corpus:</p>
<p><a href="http://4.bp.blogspot.com/-60TbnGWITW4/UyeJKC2YiGI/AAAAAAAAEUA/X2R978sF3F4/s1600/Screen+Shot+2014-03-17+at+7.45.21+PM.png"><img src="http://4.bp.blogspot.com/-60TbnGWITW4/UyeJKC2YiGI/AAAAAAAAEUA/X2R978sF3F4/s1600/Screen+Shot+2014-03-17+at+7.45.21+PM.png" /></a></p>
<p>This, if anything, massively understates the importance of BiblioBazaar to Google Books in 2008: using the 2009 data, here’s the percentage of all books (rather than words) mentioning each of those three presses:</p>
<p><a href="http://1.bp.blogspot.com/-DvKleowU3Iw/UznZ_M07r8I/AAAAAAAAEWA/pjVnvF0WzJY/s1600/BiblioBizarre.png"><img src="http://1.bp.blogspot.com/-DvKleowU3Iw/UznZ_M07r8I/AAAAAAAAEWA/pjVnvF0WzJY/s1600/BiblioBizarre.png" /></a></p>
<p>This doesn’t mean that 35% of all the books in 2000 are Macmillan, because other books will cite Macmillan in footnotes. I bet almost every university press book in the humanities and social sciences cites Harvard University Press in 1999. But given that BiblioBazaar barely existed before 2008, hardly any non-BiblioBazaar books would mention the company in 2008. So apparently, BiblioBazaar is almost 45% of the 2008 sample in Google Ngrams. That’s incredible.</p>
<p>How did “BiblioBazaar” supplant the largest presses in just one year?</p>
<p>This has messed up other sources: <a href="http://www.publishersweekly.com/pw/by-topic/industry-news/publisher-news/article/42850-bibliobazaar-how-a-company-produces-272-930-books-a-year.html">Publisher’s Weekly reported in 2010 on how BiblioBazaar leapt to the top of the Bowker’s charts</a>. In that, the company’s president is asked if they really published 270,000 books in 2009:</p>
<blockquote>
<p>“If by ‘produce’ you mean create a cover file that will print at multiple POD vendors, a book block that will print at multiple POD vendors, and metadata to sell that book in global sales channels, then yes, we did produce that many titles,” said Mitchell Davis, president of BiblioLife, parent company of BiblioBazaar.</p>
</blockquote>
<p>And what sort of books are they?</p>
<blockquote>
<p>All of the company’s content is in the public domain, and are basically “historical reprints,” Davistold <em>PW</em>, with foreign language books, and their “added layers of complexity” the fastest growing category of books. “Dealing with out-of-copyright materials lets us leverage our knowledge and relationships in the global bookselling industry more easily as we build out what is shaping up to be a pretty killer platform,” he noted.</p>
</blockquote>
<p>In other words, the entire 19th century sprang back into print. You can see this in the Ngrams charts pretty clearly: “thou”,“thee”, and”thy,” for example, shoot up fivefold in 2007-2008 after centuries of decline. I’ve noticed this before, but now I’m inclined to think most of it can be attributed to this one company.</p>
<p>Or “<a href="https://books.google.com/ngrams/graph?content=railway&amp;year_start=1850&amp;year_end=2008&amp;corpus=15&amp;smoothing=0&amp;share=&amp;direct_url=t1%3B%2Crailway%3B%2Cc0">railway</a>.” This isn’t qualitatively different from other shifts in the Ngrams database: the <a href="http://sappingattention.blogspot.com/2011/01/digital-history-and-copyright-black.html">gaps at 1922 as the library composition shifts, for example</a>. But it is quantitatively of a different order. The complete immateriality of books post-2008 means that minor decisions about whether an e-book actually exists or not can cause shifts in 40% of the corpus. I spent some time looking for words that shift at the 1922 break, and though they do exist (it seems that the loss of Harvard drops most medical terms, for example), the shifts are a few percentage points: nothing that anyone should be taking seriously in that noisy a dataset anyway.  But half the corpus: that’s something else entirely.</p>
<p>Among other strange things, that means Ngrams is almost certainly at its most useful <em>right now;</em> with each year, it gets further and further out of date, and it will be extremely hard to update it without making a lot of extremely hard choices.</p>
<h2 id="quick-postlude-i-should-hasten-to-say-that-everyone-involved-with-ngrams-is-aware-of-all-the-class-of-problems-like-this-and-that-the-quick-credibility-check-for-anyone-citing-ngrams-is-that-they-dont-use-the-post-2000-books-as-any-sort-of-evidence.-theres-a-reason-the-default-settings-stop-in-2000-and-that-the-michel-et-al-paper-urges-you-not-to-use-the-newer-data.-but-i-hadnt-realized-before-how-different-2008-was-in-particular.">Quick postlude: I should hasten to say that everyone involved with Ngrams is aware of all the class of problems like this, and that the quick credibility-check for anyone citing Ngrams is that they don’t use the post-2000 books as any sort of evidence. There’s a reason the default settings stop in 2000, and that the Michel et al paper urges you not to use the newer data. But I hadn’t realized before how different 2008 was, in particular.</h2>
<h3 id="comments">Comments:</h3>
<h4 id="wow.-i-knew-they-were-an-enormous-problem-but-i-h">Wow. I knew they were an enormous problem, but I h…</h4>
<p><a href="https://www.blogger.com/profile/02151663590594341211" title="noreply@blogger.com">Unknown</a> - <time datetime="2014-04-03T14:16:02.193-04:00">Apr 4, 2014</time></p>
<p>Wow. I knew they were an enormous problem, but I hadn’t come close to understanding the scale of the challenge they pose.</p>
<p>As bad as their polluting the data may be, there’s an even more worrisome element of their business model. The parent company partnered with libraries, offering to scan their pre-1923 titles for free in exchange for digital copies. (It’s been alleged that another publisher, Kessinger Press, was simply downloading the Google scans and repackaging them. BiblioBazaar says that’s not what it’s doing. I have no way to evaluate that claim.)</p>
<p>What’s fascinating to me is that these BiblioBazaar editions of public domain works are not available to be read on Google Books. Not as full books. Not as limited numbers of pages. Not even as text snippets. They’re simply blank. If you want them, you’ll have to buy them as print-on-demand editions off Amazon or another seller. Now, why should that be?</p>
<p>Let me offer another general and vague observation. Quite often, when there’s a BiblioBazaar edition of a work, I can’t seem to find a fully accessible version on Google Books. And in a few cases, I’ve been using a scanned pre-1923 edition of a work in research, and returned to find it no longer accessible, and noticed that there is now a BiblioBazaar edition.</p>
<p>That’s not dispositive. Google Books is screwy in a lot of ways. But there’s a track record of other publishers - most notably Kensington - allegedly filing fraudulent reports of copyright violations with Google, using its automatic processes to remove full-text editions of public domain works from Google Books. That leaves only the expensive print-on-demand editions for many titles.</p>
<p>So when I see a shady publisher slapping modern copyright dates on public domain works, and preventing anyone from seeing even snippets of the texts, I get very worried and very suspicious.</p>
<hr />
<h4 id="thats-totally-fascinating-andas-you-sayf">That’s totally fascinating, and–as you say–f…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-04-03T14:52:26.645-04:00">Apr 4, 2014</time></p>
<p>That’s totally fascinating, and–as you say–far more important than whatever it does to the data sets. I’ve only hit the minor but consistent inconvenience that the reprints show up at the top of the searches, but what you lay out seems completely plausible. Unlike YouTube infringements, which use the same basic principle, there’s no agent out there with any interest in keeping a book online besides Google itself.</p>
<p>In theory, most of those removed public-domain works should stay in Hathi and the Internet archive, which have a lot of the Google scans; I wonder if there’s some way to see how big the problem is by looking for books in the Hathi catalogue that have been removed from Google. I suspect not, because bulk access to the Google catalogue is really difficult.</p>
<hr />
<h4 id="if-ever-there-was-a-case-for-making-sure-we-unders">If ever there was a case for making sure we unders…</h4>
<p><a href="http://adamcrymble.org">Adam Crymble</a> - <time datetime="2014-04-03T14:54:27.425-04:00">Apr 4, 2014</time></p>
<p>If ever there was a case for making sure we understand our sources and data, you just made it. Very cool.</p>
<hr />
<h4 id="i-dont-suppose-this-makes-n-grams-obsolete.-m">I don’t suppose this makes n-grams obsolete. M…</h4>
<p><a href="http://adamcrymble.org">Adam Crymble</a> - <time datetime="2014-04-06T07:34:04.332-04:00">Apr 0, 2014</time></p>
<p>I don’t suppose this makes n-grams obsolete. Maybe it’s just an opportunity to acknowledge that what we really want in the datasets is first editions only. Unless we’re using the corpus for another reason. In which case we might not. It comes down to control over what’s in or what’s out though. And at the moment we really don’t have that.</p>
<hr />
<h4 id="yes-precise-control-of-contents-is-incredibly-use">Yes, precise control of contents is incredibly use…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-04-06T10:31:51.983-04:00">Apr 0, 2014</time></p>
<p>Yes, precise control of contents is incredibly useful; we (ie, Erez Aiden’s lab at Rice, which prompted Google to build the Ngram browser to begin with) and myself) have a joint proposal in at the NEH with the Hathi Trust Research Center for a Bookworm browser that would enable all sorts of custom corpora creation based on metadata.</p>
<p>But one of the things that’s most interesting about Google Ngrams is that it showed you can get a remarkably usable dataset by simply outsourcing the decisions about whether to include a book or not based on the fact of publication. (And a few other things—language and OCR quality, in particular).</p>
<p>And that paradigm, at least, is breaking down. You could say that actually, Ngrams only works through 2000 because human librarians (alive and dead) chose which books would be counted; and that at the moment it switched to publishers supplying the books, something changed. That seems possible.</p>
<hr />
<h4 id="what-we-really-want-in-the-datasets-is-first">“what we really want in the datasets is first…</h4>
<p><a href="https://www.blogger.com/profile/10049944935144722560">Unknown</a> - <time datetime="2014-04-14T14:35:04.990-04:00">Apr 1, 2014</time></p>
<p>“what we really want in the datasets is first editions only. Unless we’re using the corpus for another reason”</p>
<p>I’m not so sure about that. It betrays a certain assumption about our data use, which is that we assume ngrams-by-publication-date are a good proxy for measuring *something* culturally relevant. More specifically, that what words are written in any given year represent or are correlated to some cultural signal.</p>
<p>But (as Ben and others have brought up before) the dataset isn’t a universal sample, it’s representative of a human (in this case mostly librarian-driven) selection process. Thus, the sample isn’t necessarily one of what is written, but what is read or saved or kept for whatever reason. That selection process changes with a publisher-driven model, but it doesn’t necessarily take it any closer or farther from the original assumption.</p>
<p>Which is all to say, the ideal sample existed no more before 2000 than after, and it’s something I haven’t really seen confronted in studies that rely on these analyses. The creation process for the corpora fluctuate. I’m sure more recent books, even pre-2000, are less reliant on their popularity to have made it into ngrams than books from 300 years ago. The earlier we go, though, the less Adam’s basic claim (1st published edition is meaningful because it’s about first word use) holds up to scrutiny.</p>
<p>What’s needed, and hopefully can come out of this new project Ben mentioned, is a serious study of what exactly the signal is being processed. Is it a study of what’s being written, of what’s being read, of what’s being saved? Only then can we start saying things like “all we want are the first published edition.”</p>
<hr />
<h4 id="on-a-different-level-this-has-made-it-remarkably">On a different level, this has made it remarkably …</h4>
<p><a href="https://www.blogger.com/profile/12096376093633074580">Ilja</a> - <time datetime="2014-07-18T04:56:31.618-04:00">Jul 5, 2014</time></p>
<p>On a different level, this has made it remarkably difficult to find authentic old books. A search on Bookfinder et al. will typically only reveal an avalanche of POD junk.</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shipping maps and how states see]]></title>
        <id/>
        <link/>
        <updated>2014-03-31T18:51:00.003Z</updated>
        <content type="html"><![CDATA[<p>A map I put up a year and a half ago went viral this winter; it shows the paths taken by ships in the US Maury collection of the <a href="http://icoads.noaa.gov/">ICOADS</a> database. I’ve had several requests for higher-quality versions: I had some up already, but I <a href="https://www.flickr.com/photos/10052187@N05/13516638735/">just put up on Flickr a basically comparable high resolution version</a>. US Maury is “Deck 701” in the ICOADS collection: I also put up charts for all of the other decks with fewer than 3,000,000 points. You can page through them below, or download the high quality versions from Flickr directly. (At the time of posting, you have to click on the three dots to get through to the summaries).</p>
<p>I’ve also had a lot of questions about modern day equivalents to that chart. This, it turns out, is an absolutely fascinating question, because it forces a set of questions about what the Maury chart actually shows. Of course, on the surface, it seems to show 19th century shipping routes: that’s the primary reason it’s interesting. But it’s an obviously incomplete, obviously biased, and obviously fragmentary view of those routes. It’s a relatively complete view, on the other hand, of something more restricted but nearly as interesting: the way that the 19th century American state was able to see and take measure of the world. No one, today, needs to be told that patterns of state surveillance, data collection, and storage are immensely important. Charts like these provide an interesting and important locus for seeing how states “saw,” to commandeer a phrase from James Scott.</p>
<p>So I want to explore a couple of these decks as snapshots of state knowledge that show different periods in the ways states collected knowledge as data. In my earlier pieces on shipping, I argued that <a href="http://sappingattention.blogspot.com/2012/11/where-are-individuals-in-data-driven.html">data narratives should eschew individual stories to describe systems and collectives</a>. States are one of the most important of these collectives, and they have a way of knowing that is at once far more detailed and far more impoverished than the bureaucrats who collect for them. These data snapshots are fascinating and revealing snapshots of how the state used to and continues to pull in information from the world. (More practically, this post is also a bit of mulling over some questions for talks I’ll be giving at the University of Nebraska on April 11 and the University of Georgia on April 22st–if you’re in either area, come on down. Some of the unanswered questions may be filled in by then.)</p>
<p>So: what’s the modern analogue to the Maury map I made? The government collection from which I pulled the Maury data almost entirely consists of data from the 20th century. So rather easily I can pull out another deck that looks somewhat similar: say, the US Natl. Cntrs. for Environ. Pred. (NCEP) Ship Data, deck 892, collected between 1980 and 1997. The easiest thing to do would be just slap it up, claim it shows modern patterns of shipping (the St. Lawrence seaway! The Suez canal! The steam engine! etc.), and declare victory. And indeed I’m going to do something similar eventually with merchant marine data from the first half of the twentieth century.</p>
<p><a href="http://3.bp.blogspot.com/-SLBgT2O7Ryo/Uzlx3UBoMWI/AAAAAAAAEVQ/vtL_3NEjKYI/s1600/892.png"><img src="http://3.bp.blogspot.com/-SLBgT2O7Ryo/Uzlx3UBoMWI/AAAAAAAAEVQ/vtL_3NEjKYI/s1600/892.png" /></a></p>
<p>Deck 892: 1980-1997</p>
<p>But to make an analogue, we have to really understand what deck 701 is. It’s not a transparent representation of American shipping; rather, it’s a concerted government effort to collect data opportunistically on the areas where the least exists. Take all those Pacific routes from the Maury collection. They are, as I wrote earlier, whaling voyages. Whaling voyages, in fact, comprise about a quarter of the US Maury collection. But they were not, I feel fairly confident, a quarter of all American ships. Matthew Maury, rather, intentionally collected shipping logs from whaling vessels because he knew they were traveling to exotic locations. (The US navy, at least as far as I can tell from these logs, did nothing nearly so interesting). So the way the 19th century American state saw the map of the world was shaped by the agents at its command. (There’s a great deal of academic literature on territoriality, governmentality, and the like along these lines; history of science, history of cartography, etc.)</p>
<p>So rather than deck 892, a better analogue to the Maury data might be deck 720, from the ‘Deutscher Wetterdienst (DWD) Marine Met. Archive.’</p>
<p><a href="http://2.bp.blogspot.com/-JbuVSR8XGak/UzlyOibSUjI/AAAAAAAAEVg/KaGkRYY2pjA/s1600/720.png"><img src="http://2.bp.blogspot.com/-JbuVSR8XGak/UzlyOibSUjI/AAAAAAAAEVg/KaGkRYY2pjA/s1600/720.png" /></a></p>
<p>Deck 720, 1876-1914</p>
<p>There is a massive empty rectangle in the middle where record collection was deemed unnecessary; probably records were not collected at all, though it’s possible they were keyed in and then the punch cards corresponding to those regions was discarded. Where the state surveyed was defined by its areas of lack of knowledge. But there are other features that show, instead, limited capacity for vision and storage. The lines in the Pacific are more gridded than in the 701 chart; that’s because the data was entered onto punch cards, and rather than store out all two or three decimal points, the recorders only kept zero or one for latitude and longitude. The strange white bands running out from Texas south and west are mysterious to me. (See <a href="http://sappingattention.blogspot.com/2012/10/logbooks-and-long-history-of.html">some more details and links to yet more on how bands like these emerged</a>).</p>
<p>Of course, these initial passes didn’t yield all the data about the ocean imaginable.  And the piecemeal practice of collecting data through accidents of commercial shipping has obvious drawbacks. This still does not quite represent the modern era of data.</p>
<p>A pure drive for data collection to truly meet state needs would not rely on the accidents of commercial traffic. But what do the state’s actual priorities look like? Behold Deck 735: Russian Research Vessel (R/V) Digitisation. The age-of-sail loops from the Maury collection and the commercial beelines from the twentieth show the state vision mediated through agents of commerce who are barely responsive to central dictates. Deck 735, on the other hand, mostly collected in the late Soviet period, shows state data collection unmediated by the needs of commerce.</p>
<p><a href="http://2.bp.blogspot.com/-I8cWLLHLQ8U/Uzlxdyq_dJI/AAAAAAAAEU4/bpe4OLN2ueE/s1600/735.png"><img src="http://2.bp.blogspot.com/-I8cWLLHLQ8U/Uzlxdyq_dJI/AAAAAAAAEU4/bpe4OLN2ueE/s1600/735.png" /></a></p>
<p>Deck 735: 1936-2000</p>
<p>At first glance, the most remarkable feature is the grid: commercial vessels do not trace latitudes and longitudes, but the Soviet research program led to intense tracking over straight lines in unpopulated areas of all the oceans.</p>
<p>But they do not survey all areas equally. Each of the major ports–Murmansk, Petersburg, Sevastopol, Vladivostok–is a focus for much more intensive local exploration. But other areas as well–the US east coast, several areas of the North Atlantic, the Australian bight–fall under the state’s gaze as well, out of interest or (as in the case, I suspect, of Cuba) logistical necessity. (A note to myself: they’re not sailing to Havana–what is the port on the south? Or diplomatic provocation–they seem to be sailing to Cienfuegos rather than Havana, <a href="http://t.co/Lo8ZCWHUpv">where the USSR hoped to build a submarine base</a>)</p>
<p>There are limits, as well, on the ability of the state to see. One sharp set of lines seems to sketch out the west coast of South America; but on closer inspection, the ships are carefully keeping some number of miles off the coast. (Q: To stay in international waters, presumably? Something to do with fishing areas?)</p>
<p><a href="http://1.bp.blogspot.com/--XVSnEZ49rY/Uzl_eQNInGI/AAAAAAAAEVw/_MND6pFoaVk/s1600/735-closeup.png"><img src="http://1.bp.blogspot.com/--XVSnEZ49rY/Uzl_eQNInGI/AAAAAAAAEVw/_MND6pFoaVk/s1600/735-closeup.png" /></a></p>
<p>This is the only example I have of such powerfully gridded examination; I suspect, though, that it’s less a unique feature of some Communistic stateview and more one the random leaks out of state vaults in the period after the collapse. Somewhere there are probably comparable American, or British, or German data as well.</p>
<p>But however purely the Soviets may stand in for a modern instrumental attitude towards data, it is not an end point. Contemporary data collection is more fluid, sensor dependent, and, well, postmodern than the gridded visions of the 20th century state. Deck 715 is a beautiful early harbinger of what’s coming; ‘German Deep Drifter Data (via ISDM; originally from IfM/Univ. Kiel)’, collected in the 1980s and 1990s.</p>
<p><a href="http://1.bp.blogspot.com/-ofwgBMNNMVs/UzlxoHSHGMI/AAAAAAAAEVA/ce-EC5D1t-w/s1600/706.png"><br />
</a></p>
<p><a href="http://3.bp.blogspot.com/-Jc5qCRyf_Gw/Uzlx91abN6I/AAAAAAAAEVY/qmzLv3q1ubg/s1600/715.png"><img src="http://3.bp.blogspot.com/-Jc5qCRyf_Gw/Uzlx91abN6I/AAAAAAAAEVY/qmzLv3q1ubg/s1600/715.png" /></a></p>
<p>Deck 715</p>
<p>The outlines of the Atlantic are just barely visible (the coast of South America is best defined, for those who see nothing) because locomotion has been removed from ships altogether and <a href="http://www.bsh.de/en/Marine_data/Observations/Marine_physical_data/drifter.jsp">left in the hands of the currents</a>. All the previous versions included observations made by human beings at sea; the drifters are autonomous, mechanical devices that circle around for their natural lives before disappearing. The gridded logic of the central state appears gone. But it’s not, entirely: in video form, it appears as a mess of swirls, but the <em>insertion</em> of the capsules into the ocean frequently takes place on a rigidly gridded basis. It’s mesmerizing, and a sort of beautiful visual metaphor for the subtle dispersion of state power in postmodern societies. (You’ll probably have to come to one of those talks to see that video, at least for now). But that rigidity quickly becomes invisible; forms of observation are more flexible, more spontaneous, less dependent on the logic of central observation.</p>
<p>So: what does this have to do with larger logics of state power and surveillance? (All remaining non-academics might want to stop reading now, in case that’s not already clear.) A straightforward three part division suggests some interesting correspondences.</p>
<ol type="1">
<li><p>Weak state capacity: Opportunistic accumulation on top of existing social groups.</p></li>
<li><p>Strong state capacity: relentlessly logical, unresponsive systems of omission and collection.</p></li>
<li><p>Invisible but omnipresent; saturation of information space through soft forms of subtle collection, with state-imposed grids yielding to on-the-ground conditions to collect more omnipresent data.</p></li>
</ol>
<p>A lot about it looks like a Foucault story about the invisibility of power; but the timeframe is completely off. (The Soviet shipping route looks a lot like the gardens at Versailles.) I feel like there’s an interesting affinity to certain species of leftist political economy; you could call the last two images “From Fordism to Flexible Accumulation” (after <a href="http://www.amazon.com/The-Condition-Postmodernity-Enquiry-Cultural/dp/0631162941">David Harvey</a>) and not lose much. But Harvey’s writing about the economy more than the state; one could do this same sort of analysis on corporate records, but they tend to be more obscure and intractable than government sources. It seems like a fruitful question, at least.</p>
<h2 id="finally-an-unplaced-point-on-data-visualization.-it-strikes-me-that-ethnographic-accounts-of-state-function-and-disfunction-have-much-to-gain-from-seeing-as-clearly-as-possible-what-the-state-itself-saw-rather-than-merely-its-bureaucrats.-at-the-time-much-of-this-data-was-collected-that-was-impossible-whole-bureaucracies-were-necessary-to-make-a-map.-but-visualization-offers-a-much-more-honest-way-of-seeing-like-a-state-with-its-breadth-and-its-one-dimensionality-than-trying-to-phrase-things-into-human-narratives.-without-the-means-to-process-large-scale-data-as-efficiently-as-the-late-19th-century-american-state-it-is-very-hard-to-imagine-the-world-as-the-state-saw-it-using-such-tools-seems-to-me-essential-to-properly-understanding-its-operations.">Finally, an unplaced point on data visualization. It strikes me that ethnographic accounts of state function and disfunction have much to gain from seeing, as clearly as possible, what the state itself saw rather than merely its bureaucrats. At the time much of this data was collected, that was impossible; whole bureaucracies were necessary to make a map. But visualization offers a much more honest way of seeing like a state, with its breadth and its one-dimensionality, than trying to phrase things into human narratives. Without the means to process large-scale data as efficiently as the late-19th century American state,  it is very hard to imagine the world as the state saw it; using such tools seems to me essential to properly understanding its operations.</h2>
<h3 id="comments">Comments:</h3>
<h4 id="great-post-ben-im-a-bit-confused-by-the-id">Great post, Ben! I’m a bit confused by the id…</h4>
<p><a href="http://cameronblevins.org" title="noreply@blogger.com">Cameron Blevins</a> - <time datetime="2014-03-31T17:00:44.558-04:00">Mar 1, 2014</time></p>
<p>Great post, Ben!</p>
<p>I’m a bit confused by the idea of “what the state itself saw rather than merely its bureaucrats.” I appreciate the way this approach get at what what the state <em>tried</em> to see, but there’s a still pretty big gap between that and what the state <em>actually</em> saw or was able to see. Your visualizations are incredibly data-rich and beautiful articulations of how different kinds of space was produced by maritime movement, but I just don’t see as strong a link between them and the maps or other kinds of spatial representations used by states which, as you note, would have needed entire bureaucracies just to make a map. Or maybe there were examples of analogous maps? Either way, wonderful work!</p>
<p>-Cameron</p>
<hr />
<h4 id="youre-right-i-think-im-slightly-conflat">You’re right, I think I’m slightly conflat…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-03-31T18:21:02.111-04:00">Mar 1, 2014</time></p>
<p>You’re right, I think I’m slightly conflating two questions here; first how, and second what states see. Your question is more, I think (?) about what they see; that is, the knowledge that escapes from functionaries into data. I think one could argue that these visualizations do that to some degree ; actually, I have a paper proposal in somewhere on data revisualization that will be precisely about that (and which I actually think I may use your post office data to build one of the charts for.) Maury did publish charts that look a lot like these, though with more detail and context.</p>
<p>But here I think I should have stuck to *how* the state saw, because these are really about mechanics of vision. The analogy might be to those eyeball tracking devices that show where and how an individual reads a page without necessarily showing what they actually saw ; that gets at questions of mechanics as well as content (ie, this is more than just where they saw) without being about content exactly. (because in all these cases the what is actually wind speeds, currents, temperature, etc., not ship tracks in themselves.</p>
<hr />
<h4 id="and-come-to-think-of-it-as-a-note-to-myself-the">And come to think of it as a note to myself , the …</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-03-31T18:26:29.266-04:00">Mar 1, 2014</time></p>
<p>And come to think of it as a note to myself , the Soviets also used ship trail visualizations in the two huge 1980s atlases the Atlas arctica and Antarctica to show the paths of research vessels that are also in this collection ; the first volume of the Marine Atlas (1950-something ) doesn’t have them, but I should check the other two volumes and the1970s successor volumes</p>
<hr />
<h4 id="these-are-really-nice.-how-did-you-perform-the-vis">These are really nice. How did you perform the vis…</h4>
<p><a href="https://www.blogger.com/profile/15698968558999435841">Unknown</a> - <time datetime="2014-04-01T06:53:26.577-04:00">Apr 2, 2014</time></p>
<p>These are really nice. How did you perform the visualization?</p>
<hr />
<h4 id="mechanics-of-vision-makes-a-lot-of-sense-and-i-li">Mechanics of vision makes a lot of sense, and I li…</h4>
<p><a href="http://cameronblevins.org">Cameron Bleivns</a> - <time datetime="2014-04-01T09:02:41.333-04:00">Apr 2, 2014</time></p>
<p>Mechanics of vision makes a lot of sense, and I like the analogy to eyeball tracking devices. There seems like such a wide continuum between gathering information and making it legible. I’d love to see the Maury charts - is there a link to them anywhere?</p>
<hr />
<h4 id="here-are-a-couple-maps">Here are a couple maps:</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-04-01T09:38:01.157-04:00">Apr 2, 2014</time></p>
<p>Here are a couple maps: <a href="http://collections.lib.uwm.edu/cdm/ref/collection/agdm/id/1717">here’s a typical one</a>; <a href="http://collections.lib.uwm.edu/cdm/singleitem/collection/agdm/id/1724/rec/11">this one of the North Sea</a> is a bit less of a mess. I actually think the Naval Observatory produces really terrible graphic design compared to its contemporaries; although it sort of works, it’s hardly legible compared to stuff the Census bureau would start putting out. Not that that really matters.</p>
<hr />
<h4 id="i-made-them-in-r-using-ggplot2-libraries.">I made them in R, using ggplot2 libraries.</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-04-01T09:53:23.116-04:00">Apr 2, 2014</time></p>
<p>I made them in R, using ggplot2 libraries. <a href="https://gist.github.com/bmschmidt/9914296">Here’s the plotting code.</a>Honestly, I spent forever trying to render them in D3 so they could overlay with some animations, but just couldn’t get the svg exports to render quickly enough. So back to R.</p>
<hr />
<h4 id="love-the-visualizations-is-there-a-way-you-could">Love the visualizations!! Is there a way you could…</h4>
<p><a href="">Bob</a> - <time datetime="2014-04-02T09:32:11.112-04:00">Apr 3, 2014</time></p>
<p>Love the visualizations!! Is there a way you could share the data sets here? Also, I’m interested in your D3 work. It’s a cool library and I’d love to see what you did with it.</p>
<hr />
<h4 id="from-a-new-zealand-view-i-cant-help-wondering">From a New Zealand view I can’t help wondering…</h4>
<p><a href="https://www.blogger.com/profile/14586290254444971774">Unknown</a> - <time datetime="2014-05-10T07:01:19.975-04:00">May 6, 2014</time></p>
<p>From a New Zealand view I can’t help wondering why you don’t cut your maritime maps through the middle of Africa instead of through the Pacific!</p>
<hr />
<h4 id="maybe-something-like-this-httpeducation.nation">Maybe something like this: http://education.nation…</h4>
<p><a href="https://www.blogger.com/profile/14586290254444971774">Unknown</a> - <time datetime="2014-06-17T19:06:33.369-04:00">Jun 2, 2014</time></p>
<p>Maybe something like this:<br />
http://education.nationalgeographic.com/education/media/selecting-map-projection/?ar_a=1</p>
<hr />
<h4 id="oh-sorry-to-miss-this-yeah-i-only-cut-there-beca">Oh sorry to miss this! Yeah, I only cut there beca…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2014-06-17T19:17:07.367-04:00">Jun 2, 2014</time></p>
<p>Oh sorry to miss this! Yeah, I only cut there because there’s something I kind of like about just plotting with the straight latitude coordinates on an equirectangular projection: nothing particularly great about it.</p>
<p>I’m using <a href="http://benschmidt.org/maury2.png">just that sort of altered goode-homolosine projection for the new animated version (unreleased, so just an image here)</a>. What I’d <em>really</em> like is the Bartholomew “Lotus” projection, which only has one minor cut, but I’ve found most people find it confusing <a href="http://benschmidt.org/maurylotus.png">(slightly misshapen example here)</a></p>
<hr />
<h4 id="yes-both-very-appealing-for-maritime-uses.-maybe">Yes, both very appealing for maritime uses. Maybe …</h4>
<p><a href="https://www.blogger.com/profile/14586290254444971774">Unknown</a> - <time datetime="2014-06-21T06:29:15.712-04:00">Jun 6, 2014</time></p>
<p>Yes, both very appealing for maritime uses. Maybe not for a study of the Northwest Passage!</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Downton more traditional]]></title>
        <id/>
        <link/>
        <updated>2012-02-13T18:49:00.002Z</updated>
        <content type="html"><![CDATA[<p>[Update: I’ve consolidated all of my TV anachronisms posts at a different blog, <a href="http://www.prochronism.com/">Prochronism</a>, and new ones on Mad Men, Deadwood, Downton Abbey, and the rest are going there.]</p>
<p>Digital humanists like to talk about what insights about the past big data can bring. So in that spirit, let me talk about <em>Downton Abbey</em> for a minute. The show’s popularity has led many nitpickers to draft up lists of mistakes. Language Loggers Mark Liberman and Ben Zimmer have looked at some idioms that don’t belong for <a href="http://languagelog.ldc.upenn.edu/nll/?p=3692">Language Log</a>, <a href="http://www.npr.org/2012/02/13/146652747/im-just-sayin-there-are-anachronisms-in-downton">NPR</a> and the <a href="http://articles.boston.com/2012-02-12/ideas/31049177_1_downton-abbey-expressions-slang">Boston Globe</a>.) In the best British tradition, the Daily Mail even <a href="http://www.dailymail.co.uk/news/article-2051332/Downton-Abbey-writers-string-language-gaffes.html">managed to cast the errors</a> as a sort of scandal. But all of these have relied, so far as I can tell, on finding a phrase or two that sounds a bit off, and checking the online sources for earliest use. This resembles what historians do nowadays; go fishing in the online resources to confirm hypotheses, but never ever start from the digital sources. That would be, as the dowager countess, might say, untoward.</p>
<p>I lack such social graces. So I thought: why not just check every single line in the show for historical accuracy? Idioms are the most colorful examples, but the whole language is always changing. There must be dozens of mistakes no one else is noticing. Google has digitized so much of written language that I don’t have to rely on my ear to find what sounds wrong; a computer can do that far faster and better. So I found some copies of the Downton Abbey scripts online, and fed every single two-word phrase through the Google Ngram database to see how characteristic of the English Language, c. 1917, <em>Downton Abbey</em> really is.</p>
<p>The results surprised me. There are, certainly, quite a few pure anachronisms. Asking for phrases that appear in no English-language books between 1912 and 1921 gives a list of 34 anachronistic phrases this season. Sorted from most to least common in contemporary books, we get a rather boring list:</p>
<pre><code> \[1\] realistic prospect funding than       specialist care    pansystolic murmur
 \[5\] moment decision    the rematch        relax together     basic tips
 \[9\] a pansystolic      of randy           be defeatist       dress fittings
\[13\] dedicated nurse    wartime marriage   point pretending   fairly grand
\[17\] want grandchildren friendships out    shortages all      when peacetime
\[21\] liberal front      heavens name       staff luncheon     can posture
\[25\] major inheritance  those logic        fingerprinted or   little daydream
\[29\] very disfigured    having pancakes    taxing assignment  rationing now
\[33\] liar while         unicorn if</code></pre>
<p>Another 26 phrases do appear rarely in the 1910s, but are at least 100x as common today (sorted by biggest difference between the teens and the 1990s to least):</p>
<pre><code> \[1\] black market      the basics        overall charge    there anymore
 \[5\] feel loved        work load         most dedicated    ganging up
 \[9\] gonna need        first priority    her homework      our funding
\[13\] you anymore       bit carried       hospital costs    likely outcome
\[17\] off limits        contact her       more traditional  exercise classes
\[21\] from scratch      in overall        current situation guest bedroom
\[25\] you gonna</code></pre>
<p>A few of these are just rare words, plausible <em><a href="http://en.wikipedia.org/wiki/Hapax_legomenon">hapax legomena</a></em> in the time period. But others are egregious, howling mistakes. We see here several of the phrases Zimmer discusses (‘those logic pills’, ‘the rematch’,‘contact her’ for ‘get in touch with’). There are also some more obvious anachronisms (‘fingerprint’ as a verb, ‘did her homework’ as a metaphor for being prepared) and a few less recognizably modern phrasings like “<a href="http://books.google.com/ngrams/graph?content=realistic+prospect&amp;year_start=1800&amp;year_end=2000&amp;corpus=0&amp;smoothing=3">realistic prospect</a>” (which, when you think about it, is quite a mixed metaphor) and “dress fittings.” Expanding it a bit reveals some more howlers: Lord Downton’s complain that his family is “ganging up” on him (the OED has it as a 1925 American coinage); Lady Mary’s concern about losing the “moral high ground” to Sybil (<a href="http://books.google.com/ngrams/graph?content=moral+high+ground&amp;year_start=1800&amp;year_end=2000&amp;corpus=0&amp;smoothing=3">a creation of the 60s that didn’t really take off until the early 1980s</a>); a usage of the Americanism “cow pie” when a Briton would have said ‘cow pat;’ and several more. None of those sound as jarring, but they are equally inaccurate. (I particularly like ‘cow pie’; we tend to think that rural language is eternal, but it can change as easily as city terms).</p>
<p>With the full list, we can see some broader patterns of error. There are some areas where writers persistently drop the ball. Through much of season 2, Downton Abbey is a hospital or convalescent home, and medical vocabulary presents a particularly problem. Branson escapes the draft because of a “mitral valve prolapse” (first use, c. 1965) causing a “pansystolic murmur” (c. 1953); both terms suggest St. Elsewhere more than the Great War. The doctor’s helpers aren’t trained in ‘specialist care’; hardly their fault, since the phrase was never used before 1925. The household is relieved that Carson the butler did not suffer a ‘heart attack’; but that phrase was about 50x rarer in 1917 (perhaps a coronary, like the one that nearly killed Roger Sterling in season 1 of <em>Mad Men</em>, would have been more appropriate?) and, so far as I can tell, only an ‘acute heart attack’ would have meant myocardial infarction to the crew at Downton.</p>
<p>Season 2’s Great War setting opens the door for another sort of mistake: words from the Second World War showing up 20 years ahead of schedule. To most of us, the lingo from the wars is indistinguishable; but there are some major mistakes. One subplot involves Thomas setting up business selling goods on the black market; “there are shortages all around,” he declares. He might as well be speaking Greek: the ‘<a href="http://books.google.com/ngrams/graph?content=black+market&amp;year_start=1800&amp;year_end=2000&amp;corpus=0&amp;smoothing=3">black market</a>’ doesn’t emerge until 1941, and though  businessmen (particular Americans) sometimes used ‘shortages’ as the opposite of ‘surpluses,’ <a href="http://bookworm.culturomics.org/?%7B%22query%22%3A%7B%22index%22%3A0%2C%22time_measure%22%3A%22year%22%2C%22time_limits%22%3A%5B1815%2C1922%5D%2C%22counttype%22%3A%22Percentage_of_Books%22%2C%22words_collation%22%3A%22Case_Sensitive%22%2C%22smoothingSpan%22%3A%225%22%2C%22search_limits%22%3A%5B%7B%22word%22%3A%5B%22shortages%22%5D%2C%22country%22%3A%5B%22UK%22%5D%2C%22lc1%22%3A%5B%22PR%22%2C%22PS%22%2C%22PZ%22%5D%7D%2C%7B%22word%22%3A%5B%22shortages%22%5D%2C%22country%22%3A%5B%22USA%22%5D%2C%22lc0%22%3A%5B%22H%22%5D%7D%2C%7B%22word%22%3A%5B%22shortages%22%5D%2C%22country%22%3A%5B%22USA%22%5D%2C%22lc1%22%3A%5B%22PR%22%2C%22PS%22%2C%22PZ%22%5D%7D%5D%7D%2C%22terms%22%3A%5B%22shortages%22%5D%2C%22category_data%22%3A%5B%5B%5B%22country%22%2C%5B%22UK%22%5D%5D%2C%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%22PR%22%2C%22PS%22%2C%22PZ%22%5D%5D%2C%5B%22LCSH%22%2C%5B%5D%5D%2C%5B%22aLanguage%22%2C%5B%5D%5D%5D%2C%5B%5B%22country%22%2C%5B%22USA%22%5D%5D%2C%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22H%22%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22LCSH%22%2C%5B%5D%5D%2C%5B%22aLanguage%22%2C%5B%5D%5D%5D%2C%5B%5B%22country%22%2C%5B%22USA%22%5D%5D%2C%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%22PR%22%2C%22PS%22%2C%22PZ%22%5D%5D%2C%5B%22LCSH%22%2C%5B%5D%5D%2C%5B%22aLanguage%22%2C%5B%5D%5D%5D%5D%2C%22comparison%22%3A%22texts%22%7D">it is so rare in British speech that it almost never appears in UK fiction from the period</a>. “In short supply,” also used in this subplot, was about 250 times as common during the second world war as during the first. Even the today ubiquitous ideas of ‘wartime’ and ‘peacetime’ aren’t appropriate; Mrs. Bryant refers to a ‘wartime marriage;’ but the use of ‘wartime’ and ‘peacetime’ as adjectives didn’t pick up in earnest until 1941.</p>
<p>But we can do more than just pick nits on idiomatic speech. (“Pick nits” is <a href="http://books.google.com/ngrams/graph?content=pick+nits%2Cnitpick&amp;year_start=1800&amp;year_end=2000&amp;corpus=0&amp;smoothing=3">1960</a>, by the way). It lets us look more generally at what the show gets right and wrong about past language. Every episode has dozens of lines that are just slightly off, and it’s in these that the patterns really look funny. In addition to the 60 phrases above, there are another 260 that are at least 10 times more common in the 1990s than in the 1910s. These arephrases like “at long last,” “from scratch”, and “act fast”–maybe a few could be spoken in the teens, but all of them together?</p>
<p>Some of these are extremely common. To help me find the words, I asked R to make a chart that looks like this to find the worst mistakes in every episode of the season. (This is last night’s: click to enlarge):</p>
<p><a href="http://2.bp.blogspot.com/-5pGBXJFpf3A/TzlRjDb6iSI/AAAAAAAAC_Q/CVwfubbuqGM/s1600/Downton+Abbey+unlikely+words.png"><img src="http://2.bp.blogspot.com/-5pGBXJFpf3A/TzlRjDb6iSI/AAAAAAAAC_Q/CVwfubbuqGM/s640/Downton+Abbey+unlikely+words.png" /></a></p>
<p>[ed–If you want to see more, <a href="http://sappingattention.blogspot.com/2012/02/downton-abbey-anachronisms-season.html">they’re all in my next post on the topic</a>.]</p>
<p>Farther to the left means less common nowadays; higher up means more common today (‘be defeatist’ is next to 3; it’s 10^3, or 1000x as common today) and below 0 means more common in 1917. Looking at these, the new words on the upper left jump out, but some more common words that are only overused 5 or 10 times jump out as well.</p>
<p>For example: Characters in Downton Abbey say “I must” 24 times, three times as often as they say “I need to.” Books from the period, on the other hand, say “I must” <em>three hundred times</em> as often; going by the printed literature, the Abbey’s residents should “need to” do something about once every ten seasons, not once an episode. Ben Zimmer pointed out that some characters say “I’m just saying” anachronistically, but it’s not just that phrase: they use “just” to modify meaning far too much. Words like “just wrong,” “just sucking,” “just need” are frequent, and uncharacteristic. (They should be saying “only wrong,” I think).</p>
<p>This is not to say that they get everything wrong. The writers tune their ears well enough to get quite a bit right. They know to say “sympathy with” rather than “sympathy for,” and so on. They know to use “awfully” as an intensifier, and so on. We can find the shining examples of period language in Downton, too: all of these phrases were at least 6x as common in the teens as today:</p>
<pre><code> \[1\] this war               the trenches           who shall
 \[4\] war has                practically a          so slight
 \[7\] old chap               newspaper man          civilised world
\[10\] dressing station       very feeble            for luncheon
\[13\] little chap            jolly well             stand well
\[16\] wonderful what         from Arras             1914 I
\[19\] thither as             before luncheon        our chauffeur
\[22\] soldier servant        a plutocrat            no livery
\[25\] whole bally            awfully cut            tremendous disturbance
\[28\] hereafter forever      wee chap               me enlist
\[31\] bally lot              hall boys              our Arch
\[34\] dressing gong          seems jolly            little waspish
\[37\] no convalescence       shining film           beggared if</code></pre>
<p>In all, the language in Downton is about 50-50; half is more common in 1995, half more common in 1917. In the abstract, that doesn’t sound great to me, but we need a basis for comparison. Let’s take another show: the beloved “Pride and Prejudice” adaptation from the nineties.</p>
<p><a href="http://3.bp.blogspot.com/-R47EvaOZqOk/TzlUIPBDdAI/AAAAAAAAC_Y/c9d1g334qYk/s1600/Pride+and+Prejudice+example.png"><img src="http://3.bp.blogspot.com/-R47EvaOZqOk/TzlUIPBDdAI/AAAAAAAAC_Y/c9d1g334qYk/s640/Pride+and+Prejudice+example.png" /></a></p>
<p>That band on the left are the completely new phrases from 1815 to 1995; a lot of language doesn’t appear at all in books from Austen’s time. Now, <em>Pride and Prejudice</em> has a lot of obstacles to overcome; the books are worse, a lot of occurrences of ‘someone’ will appear as ‘fomeone’ in the old OCR, and it’s set 100 years earlier than Downton. But nonetheless, the center of that cloud is a little lower than Downton’s. Of the 6 episodes, between 60 and 67% of the words are more common in 1815 than in 1995; for Downton, only about 50% are more common. That’s because the BBC could steal lines from Austen that <em>sounded authentic</em> even without the writers having to think up phrases like “total want of” or “cordially wish.” If you care only about gross anachronisms, <em>Pride and Prejudice</em> will sound worse than Downton because from time to time they added words that Austen <em>didn’t</em> write; but if you care about historically accuracy overall, you’ll get a much better experience of old-fashioned speech from the show that took from Austen.</p>
<p>For a script without a source base to crib from, though, Downton doesn’t do so poorly. A couple episodes of <em>Mad Men</em> I checked were possibly worse <em>[Ed.–Looking into it a little more, I take this back; they’re probably better]</em>; even great novelists do no better. Edith Wharton’s “The Age of Innocence” is one of the great historical novels in the public domain (written in 1921, set in the 1870s), but dialogue in it routinely uses phrases like “marked trend” and “shoe polish” that no one in the 1870s would have known. In fact, only 40% of its words are more common in the 1870s than the 1920s; even worse than Downton. Of course, they all sound old-fashioned to us now.</p>
<p>Do these mistakes really matter? Yes and no. Maybe the characters say it best:</p>
<pre><code>ROBERT, EARL OF GRANTHAM
You don&#39;t think she&#39;d be happier with a more traditional set up?</code></pre>
<p>Nothing seems out of order here, perhaps. But, “<a href="http://books.google.com/ngrams/graph?content=more+traditional&amp;year_start=1800&amp;year_end=2000&amp;corpus=0&amp;smoothing=3">more traditional</a>” is a <em>profoundly</em> untraditional way of describing things. Historians know that the “<a href="http://scholar.google.com/scholar?q=Invention+of+tradition&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart&amp;sa=X&amp;ei=T0s5T6DrHMPv0gHZtqjIAg&amp;ved=0CBgQgQMwAA">invention of tradition</a>” was rampant in Victorian England; the practice of happily talking about “more traditional” and “less traditional” outcomes <a href="http://books.google.com/ngrams/graph?content=more+traditional%2Ctraditionally&amp;year_start=1800&amp;year_end=2000&amp;corpus=0&amp;smoothing=3">is even more recent</a>. To a real Earl of Grantham, talking about tradition as a sliding scale would rather miss the point; either it’s traditional or it’s not.</p>
<p>But today, of course, those shades of tradition–sometimes right, sometimes wrong–exactly what the show is about. We think we can recapture it in little parts; that various characters in the past can stand in for us, and that we might behave just like they do.</p>
<h2 id="this-is-the-real-weakness-of-downton-abbey-id-say.-not-just-the-language-but-the-sensibilities-are-obviously-modern-easy-for-us-to-understand-and-false-to-the-reality-of-the-past.-i-admit-i-skipped-large-parts-of-the-second-season-of-downton-abbey-to-watch-cheers-which-gives-a-far-more-nuanced-depiction-of-the-way-social-class-is-used-as-in-instrument-of-authority-and-liberation-than-downton.-but-to-imagine-yourself-sophisticated-fighting-prejudice-and-eating-quaint-food-downtons-just-the-thing.">This is the real weakness of Downton Abbey, I’d say. Not just the language but the sensibilities are obviously modern, easy for us to understand, and false to the reality of the past. (I admit I skipped large parts of the second season of Downton Abbey to watch <em>Cheers</em>, which gives a far more nuanced depiction of the way social class is used as in instrument of authority and liberation than <em>Downton</em>.) But to imagine yourself sophisticated, fighting prejudice and eating quaint food, Downton’s just the thing.</h2>
<h3 id="comments">Comments:</h3>
<h4 id="showing-my-age-perhaps-but-how-does-upstairs">Showing my age, perhaps, but how does <em>Upstairs,…</em></h4>
<p><a href="#">Anonymous</a> - <time datetime="2012-02-13T22:47:59.388-05:00">Feb 1, 2012</time></p>
<p>Showing my age, perhaps, but how does <em>Upstairs, Downstairs</em> compare to <em>Downton Abbey</em>? They’re both in Edwardian settings, but the standards for period correctness (and the means to detect them) weren’t as available when <em>Upstairs, Downstairs</em> was produced. At the very least, we can expect few to no 1980s or 90s coinages in <em>Upstairs, Downstairs.</em></p>
<hr />
<h4 id="want-grandchildren-is-clearly-the-best">“Want grandchildren” is clearly the best…</h4>
<p><a href="https://www.blogger.com/profile/13542022273476075921">Jamie</a> - <time datetime="2012-02-14T00:10:13.132-05:00">Feb 2, 2012</time></p>
<p>“Want grandchildren” is clearly the best thing that no one said.</p>
<hr />
<h4 id="also-roseanne-was-a-much-better-show-about-class">Also, Roseanne was a much better show about class …</h4>
<p><a href="https://www.blogger.com/profile/13542022273476075921">Jamie</a> - <time datetime="2012-02-14T00:19:44.313-05:00">Feb 2, 2012</time></p>
<p>Also, Roseanne was a much better show about class than Cheers was! It is also on Netflix instant watch.</p>
<hr />
<h4 id="exactly.">Exactly.</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-14T11:05:07.828-05:00">Feb 2, 2012</time></p>
<p>Exactly.</p>
<hr />
<h4 id="some-of-these-comments-see-to-assume-that-class-re">Some of these comments see to assume that class re…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-14T11:56:12.499-05:00">Feb 2, 2012</time></p>
<p>Some of these comments see to assume that class relationships in the US were/are identical to those in the UK in the early 1900’s, this is I think highly questionable.</p>
<hr />
<h4 id="would-ill-be-a-monkeys-uncle">Would “I’ll be a monkey’s uncle”…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-14T13:42:57.400-05:00">Feb 2, 2012</time></p>
<p>Would “I’ll be a monkey’s uncle” be one of them. Thought that got popularized during Scopes’ Monkey Trial?</p>
<hr />
<h4 id="i-think-i-believe-this-about-roseanne-but-im">I think I believe this about Roseanne, but I’m…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-14T13:51:06.195-05:00">Feb 2, 2012</time></p>
<p>I think I believe this about Roseanne, but I’m still probably not going to watch it.</p>
<hr />
<h4 id="i-think-i-might-look-at-a-few-more-shows-nextthe">I think I might look at a few more shows next–the…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-14T13:52:28.485-05:00">Feb 2, 2012</time></p>
<p>I think I might look at a few more shows next–the only problem with Upstairs, Downstairs is that I haven’t seen it myself, and I might pick up more of the plot than I want…</p>
<hr />
<h4 id="i-certainly-dont-want-to-claim-theyre-th">I certainly don’t want to claim they’re th…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-14T14:00:18.224-05:00">Feb 2, 2012</time></p>
<p>I certainly don’t want to claim they’re the same. I think I didn’t say what I meant clearly here. Which is: a major part of class differences are in tastes, priorities, language, and so on; we find people of other social classes ridiculous, distasteful, or foreign in ways that cut both up and down. But in Downton, people are rich and poor, elevated and cast down, but they almost always relate to each other so easily; they have a good laugh when Sybil doesn’t know how to make tea, but different ideas about what’s important never seem to divide them in really important ways. Or at least, that’s what I’ve felt a lot of the time watching it.</p>
<hr />
<h4 id="im-not-sure-a-5050-mix-of-period-and-modern">I’m not sure a 50/50 mix of period and modern …</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-15T08:04:51.599-05:00">Feb 3, 2012</time></p>
<p>I’m not sure a 50/50 mix of period and modern makes it “quite good”, most of the examples you give for period language are either (a) things that involve talking about the war (which as you observe they do using modern language) or extremely stereotypical costume-drama-language (like “the whole bally lot”).</p>
<p>A line like “In 1914 I sold the whole bally lot on the black market” would be two-thirds period, one third ahistorical, but it’s still something nobody in 1916 would *say*.</p>
<p>That said, I don’t think it’s any more desirable for the cast of Downton to speak in the language of 1916 than it is for Macbeth to speak in an authentically Scots dialect.</p>
<p>- Dan Hemmens</p>
<hr />
<h4 id="section"></h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-15T11:24:27.285-05:00">Feb 3, 2012</time></p>
<p><a href="http://books.google.com/ngrams/graph?content=monkey%27s+uncle&amp;year_start=1900&amp;year_end=2000&amp;corpus=5&amp;smoothing=0">Looks like one to me.</a> (My algorithm’s didn’t catch it because I skip apostrophes.) There a decent chance <a href="http://books.google.com/ngrams/graph?content=monkey%27s+uncle&amp;year_start=1900&amp;year_end=2000&amp;corpus=6&amp;smoothing=0">it comes even later into UK English, though that could just be a result of book size or genre composition.</a></p>
<hr />
<h4 id="fair-points.-though-the-extreme-outliers-are-just">Fair points. Though the extreme outliers are just …</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-15T11:49:33.009-05:00">Feb 3, 2012</time></p>
<p>Fair points. Though the extreme outliers are just as you say, my initial impression of the less extreme ones is that Fellowes also does a decent job sprinkling in slightly musty phrasings throughout. I don’t have a great statistical model right now, but I think of it this way: you’d expect, knowing nothing else, that half the words that come out of my mouth today are getting more popular, and half getting less popular. (More than a third of the dialogue in a Marx Brothers movie is more common today than in 1933, for example. A perfect period drama would hit that percentage, but I think it’s a pretty much impossible goal.) I don’t want people to think that Downton is so much worse than Upstairs-Downstairs, or Remains of the Day, or whatever; it’s far more interesting to me that <em>no one</em> can write perfectly authentic period dialogue.</p>
<p>Macbeth raises an interesting point. Of course today Shakespeare’s language is perfect. But the histories, we all know, are massively distorted for blatantly political reasons. Were I blogging in 1605, I’m pretty sure I’d be livid about the pasting he gives to Richard III or the whitewashing to Henry IV. And that political purpose was enabled by making their language so transparent to the groundlings. On the other hand, I wouldn’t be surprised if Shakespeare’s audiences were more attuned to the obvious contemporary political valences of those plays than American viewers are to those in Downton.</p>
<hr />
<h4 id="this-was-fascinating-but-i-am-left-with-one-quest">This was fascinating, but I am left with one quest…</h4>
<p><a href="">Matt</a> - <time datetime="2012-02-15T15:09:53.964-05:00">Feb 3, 2012</time></p>
<p>This was fascinating, but I am left with one question. If the show is so wildly popular, are nitpickers actually listing ‘mistakes’? I suppose I just wonder exactly what you have quantified about the show.</p>
<p>If a 50-50 split of period /modern language is a deliberate choice and is also the recipe for a more engaged audience, it seems it’s more a measure of success (for a television show) rather than of failure (for a documentary).</p>
<hr />
<h4 id="i-think-were-more-or-less-in-agreement-here">I think we’re more or less in agreement here, …</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-16T08:17:16.660-05:00">Feb 4, 2012</time></p>
<p>I think we’re more or less in agreement here, and I think “slightly musty phrasings” are really all you want in a period drama (it’s written for a modern audience, the fact that it would sound strange and alien to somebody from the actual time period is completely beside the point). Downton does a reasonably good job of sounding past-ey, but I don’t think it particularly tries to recreate the language of 1916, and I’m not particularly convinced it should.</p>
<p>As for Shakespeare - obviously the Histories were political, but nobody cares at all that they’re not written in period language. You might have complained, legitimately, that Shakespeare did a ridiculous hatchet job on Richard the Third, but nobody would have complained that the play opens with “Now is the winter of our discontent…” despite the fact that the word “discontent” wasn’t in common usage during the Wars of the Roses.</p>
<p>- Dan Hemmens</p>
<hr />
<h4 id="great-post.-the-question-of-periodicity-is-somethi">Great post. The question of periodicity is somethi…</h4>
<p><a href="">Paul Davis</a> - <time datetime="2012-02-16T11:19:56.822-05:00">Feb 4, 2012</time></p>
<p>Great post. The question of periodicity is something I’m interested in with relation to the eighteenth-century stage and I think it’s always an open question of how historically accurate you want your language and the sensibilities of your characters to be when writing a historical drama for a contemporary audience (e.g. The ideas and emotions expressed by the characters in Malick’s The New World seem extremely anachronistic in comparison to those expressed in, say, Weir’s Master and Commander, but I think those anachronisms also make it a more compelling film in certain ways).</p>
<p>On a more practical level, I just wanted to note that this might be an example of how digital humanities could demonstrate its utility for broader society (i.e. sell out). In order to avoid the embarassment of those articles about notable howlers, I can easily imagine the screenwriters for shows like Downton running drafts of their scripts through the sort of analysis you’ve done and editing various lines accordingly. Maybe you should offer to do it for them in exchange for generous compensation from the studio.</p>
<p>A bit less ambitious than uncovering the structure of language through ‘Age cohort and vocabulary use’ but probably more lucrative.</p>
<hr />
<h4 id="wouldnt-shakespeare-or-any-other-creative-wri">Wouldn’t Shakespeare or any other creative wri…</h4>
<p><a href="">Maneki Nekko</a> - <time datetime="2012-02-16T22:35:37.712-05:00">Feb 4, 2012</time></p>
<p>Wouldn’t Shakespeare or any other creative writer be anachronistic in the sense described here? For instance, the phrase “a sea change” appears much more often in 19th and 20th century writing than it did among Shakespeare’s contemporaries.</p>
<hr />
<h4 id="danfair-enough.-i-think-youre-on-stronger"><span class="citation" data-cites="Dan">@Dan</span>–Fair enough. I think you’re on stronger …</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-17T12:14:55.603-05:00">Feb 5, 2012</time></p>
<p><span class="citation" data-cites="Dan">@Dan</span>–Fair enough. I think you’re on stronger ground than I about Shakespeare here; I might well have complained about ‘discontent,’ but it would have been a pretty absurd point to make. The real question is how important we think accuracy is in any context. For some scripts (eg Shakespeare, Deadwood) I don’t care at all if the language is accurate, but for some others (eg Mad Men) the idea that the creator is trying scrupulously to be accurate is a big part of the appeal for some people.</p>
<p><span class="citation" data-cites="Maneki">@Maneki</span>–Though Shakespeare did introduce several phrases that would show up as anachronisms, so much else of his language (any two word phrase starting or ending with ‘thou’, for example) would be obviously fusty that it doesn’t. Shakespeare, I suspect, is about the only English writer who inflected the language enough that his own neologisms could make a difference.</p>
<p>I actually ran some old scripts through the algorithm to check this very question; “Duck Soup” (1933) and “The Apartment” (1960) do come across as less anachronistic than Downton or Mad Men. (Although it’s hard to compare between time periods, which is why I retracted my claim about Mad Men being worse than Downton–I should post separately about this, but in short, I now think it’s better).</p>
<hr />
<h4 id="good-point-and-i-dont-have-a-definite-answer">Good point, and I don’t have a definite answer…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-17T12:24:32.869-05:00">Feb 5, 2012</time></p>
<p>Good point, and I don’t have a definite answer. There might be some magic formula for how musty to get, I guess. It’s hard to say what makes a show resonate–I suspect that the modern speech in Pride and Prejudice wasn’t more engaging than the stuff ported over from Austen, but maybe I’m wrong. Certainly some sorts of anachronisms, like the one’s Zimmer talks about, make it harder to watch the show. (And even some, like ‘suck up’, that might <em>not</em> be anachronisms, can feel too much like them).</p>
<p>For me personally, I like it when TV/movie creators set the bar as high as possible for being both accessible and period-correct. But not everyone needs to agree. Some people used to like their plays to be written in iambic pentameter, too, but most people now would just find it a silly hoop.</p>
<hr />
<h4 id="is-this-sort-of-thing-on-a-far-less-detailed-basi">Is this sort of thing (on a far less detailed basi…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-24T19:23:40.112-05:00">Feb 5, 2012</time></p>
<p>Is this sort of thing (on a far less detailed basis) possible to do with existing, free resources on the internet?</p>
<hr />
<h4 id="i-think-one-runs-the-risk-of-losing-ones-audi">I think one runs the risk of losing one’s audi…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-03-03T16:28:08.606-05:00">Mar 6, 2012</time></p>
<p>I think one runs the risk of losing one’s audience if the language is too authentic in such a period script as “Downton Abbey”. It has to sound authentic “enough”, yet be accessible to the ear of the modern audience. Most viewers aren’t going to be researching the language bits anyhow. Now if the Earl of Grantham were using an iPhone…</p>
<hr />
<h4 id="great-post-how-do-you-account-for-the-differences">Great post! How do you account for the differences…</h4>
<p><a href="https://www.blogger.com/profile/10680264838076089896">dashore</a> - <time datetime="2012-04-04T11:55:33.964-04:00">Apr 3, 2012</time></p>
<p>Great post! How do you account for the differences between spoken and written language? It is very likely that people in the teens and twenties said words and phrases that they never would have thought to write down.</p>
<p>In other words, your measures compare written utterances to spoken utterances, and consequently overestimate the frequency of anachronism. Is there a way to correct for this? You could, for example, test Downton against a corpus of works that are records of spoken utterances or, as in playtexts, attempt to imitate spoken utterances. This would reduce the size of your corpus significantly, and would thus make for more “pure anachronisms,” but it might also yield closer correspondence more generally.</p>
<hr />
<h4 id="thanks-there-are-definitely-differences-between-w">Thanks! There are definitely differences between w…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-04-04T12:19:26.501-04:00">Apr 3, 2012</time></p>
<p>Thanks! There are definitely differences between written and spoken. <a href="http://www.prochronism.com/2012/02/downton-abbey-anachronisms-season.html">In my other post on Downton Abbey</a>, I look at a Shaw play set in a country manor to get some baseline for at least imitated speech, and it shows very different patterns: I think that (seeing whether contemporary imitations of 1920s speech and 1920s imitations of 1920s speech) is the sweet spot here.</p>
<p>In general, I think it’s easy to overstate the difference between vocabulary (as opposed to syntax, or usage rates) between spoken and written English. For later periods (like Mad Men) there are some actual speech recordings that should be interesting. But I haven’t fully explored those yet. Someday!</p>
<hr />
<h4 id="great-analysis-ben-i-was-wondering-if-you-would-b">Great analysis Ben! I was wondering if you would b…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-04-07T22:00:46.062-04:00">Apr 6, 2012</time></p>
<p>Great analysis Ben! I was wondering if you would be able to help me locate Downton Abbey scripts online as you said you found some. I can’t for the life of me find any, but perhaps my resources need expanding.<br />
Where did you find yours?!<br />
Your help very much appreciated!<br />
thanks:)</p>
<hr />
<h4 id="for-this-post-i-used-some-ones">For this post I used some ones</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-04-07T23:53:30.705-04:00">Apr 6, 2012</time></p>
<p>For this post I used some ones <a href="http://scriptline.livejournal.com/61759.html">someone put up at scriptline.livejournal.com;</a> more recently, I’ve mostly been using <a href="http://subscene.com/Downton-Abbey-Second-Season/subtitles-90679.aspx">subtitles/closed captions,</a> which can have transcription errors, but if you get the right ones don’t include any non-dialogue, and it’s easy to get lots of scripts.</p>
<p>On the other hand, they also don’t include character names, which can make them bad for anything other than looking at words actually spoken.</p>
<hr />
<h4 id="thanks-so-much-will-try-both-options-i-was-looki">Thanks so much, will try both options! I was looki…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-04-08T15:07:50.358-04:00">Apr 0, 2012</time></p>
<p>Thanks so much, will try both options! I was looking more for actual scripts, but this is a start. Appreciate your help:)</p>
<hr />
<h4 id="what-strikes-me-is-how-much-of-the-inaccuracy-is-a">What strikes me is how much of the inaccuracy is a…</h4>
<p><a href="http://handworn.livejournal.com">Handworn</a> - <time datetime="2012-07-09T13:59:53.291-04:00">Jul 1, 2012</time></p>
<p>What strikes me is how much of the inaccuracy is a matter of degree. “High ground” is a long-established military concept, and it doesn’t take much inventiveness to add moral to it in the military parallel Lady Mary was drawing. And “traditional” might have been a new adjective then, relatively speaking, but turning an adjective into a comparative is about as surprising as buying a car and then taking it for a spin (so to speak). These are particularly erudite and intelligent people (at least, Upstairs) and it doesn’t strain credulity that they’d coin relatively inevitable neologisms from time to time to suit a need, the fact that those phrases wouldn’t generally catch on for some time notwithstanding. What does strain credulity is that they’d do it so often.</p>
<hr />
<h4 id="what-this-analysis-doesnt-take-into-account-i">What this analysis doesn’t take into account i…</h4>
<p><a href="">Anonymous</a> - <time datetime="2013-12-27T18:25:57.463-05:00">Dec 5, 2013</time></p>
<p>What this analysis doesn’t take into account is the earlier use of these words, phrases and idioms in spoken language, before they show up in printed literature. Also, literature prior to WWI was almost exclusively written by people from the middle and upper classes and so doesn’t fully reflect the rich working-class vocabulary of the day.</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixing the job market in two modest steps]]></title>
        <id/>
        <link/>
        <updated>2012-01-30T15:20:00.000Z</updated>
        <content type="html"><![CDATA[<p>Another January, another set of hand-wringing about the humanities job market. So, allow me a brief departure from the digital humanities. First, in four paragraphs, the problem with our current understanding of the history job market; and then, in several more, the solution.</p>
<p>Tony Grafton and Jim Grossman launched the latest exchange with what they call a “<a href="http://www.historians.org/Perspectives/issues/2011/1110/1110pre1.cfm">modest proposal</a>” for expanding professional opportunities for historians. <a href="http://hnn.us/articles/history-worth-fighting-where-aha">Jesse Lemisch</a> counters that we need to think bigger and mobilize political action. There’s a big and productive disagreement there, but also a deep similarity: both agree there isn’t funding inside the academy for history PhDs to find work, but think we ought to be able to get our hands on money controlled by someone else. Political pressure and encouraging words will unlock vast employment opportunities in the world of museums, archives, and other public history (Grafton) or government funded jobs programs (Lemisch). These are funny places to look for growth in a 21st-century OECD country (perhaps Bill Cronon could take the more obvious route, and make his signature initiative as AHA president creating new tenure-track jobs in the <a href="http://www.economist.com/node/14845197">BRICs</a>?) but the higher levels of the profession don’t see much choice but to change the world.</p>
<p>Like most non-combatants, I tend to agree in general with the concrete aims of both Lemisch and Grafton. Still, there’s something I always find too transparently self-serving about their arguments. Contra Grafton and Grossman’s title, they’re not modest at all. Both stances can actually seem quite grandiose in their claims for history. If only we had the courage to proclaim how important history is, Congress would fund it; if only we were clearer about the immense benefit a Ph.D. education can give to someone seeking alt-ac employment, the world would beat a path to our doors. Never mind that the size of the profession is set not by idealism, but by a combination of university economics and a close attention to market share; every family member is a blessing, and we must care for them all. The fault may not lie in the stars, but the only fault in ourselves is that we are underlings.</p>
<p>There used to be an alternative that didn’t take ourselves so seriously: let’s call it contractionism. It called for the immediate limiting of new entrants to Ph.D. programs, and a continuing balance of Ph.D. numbers with new tenure track jobs. Thanks largely to Rob Townsend’s valiant data collecting at the AHA and chart-making in <em>Perspectives</em>, this once seemed like a serious plan held back only by collective-action problems. But no longer. It probably gives Marc Bousquet too much credit to say he single-handedly made contractionism disreputable in early 2010. But since his series of posts, originally against Townsend, making arguments against it alternately flawed (<a href="http://chronicle.com/blogs/brainstorm/at-the-aha-huh/19544">contractionism is discredited because, like Ronald Reagan, it focuses on the supply side</a>)* and incisive (<a href="http://howtheuniversityworks.com/wordpress/archives/239">it would take 20 years for contraction to filter to the job market</a>), support for limiting the number of Ph.D.s seems weakened. (My own program, which is also Grafton’s, enrolled 31 new students last year, up from 25 before the financial crisis; numbers across the board, though, are slightly down.)</p>
<p><em>*I would think that if anything is supply-side economics, it’s the conviction that indeterminate production of Ph.D.s should be expected to create its own demand.</em></p>
<p>Even if contractionism won’t work, I miss its modesty. So let me put forward a new proposal. It’s quite a simple two-step plan. It involves only money we have, and doesn’t rely on others creating miraculous new opportunities. (Though once we get that going properly, it could work hand in hand with those.) If we <em>did</em> enact both of these reforms, we’d be far better off than we are now, and quite soon. If you know anything equally innocent, cheap, easy, and effectual, let’s hear it.</p>
<p>Step 1: <strong>Eliminate all post-docs.</strong></p>
<p>The job ‘crisis’ is now over thirty years old, but two things have changed in the last fifteen. One was the provision of living-wage stipends to large numbers of enrolled graduate students, which was a basically unalloyed good. The other was the appearance of post-doctoral opportunities, previously mostly confined to the sciences. I think a lot of humanists think (or more accurately, feel) that setting up a new postdoctoral program helps the system in the same way that higher grad stipends do. Not so.</p>
<p>To the contrary: they create a disastrous incentive structure that undoes the good of higher stipends. In the science, this has led to a situation where <a href="http://www.miller-mccune.com/science/the-real-science-gap-16191/">the only sane reason to stay on the academic track is the desire for a green card</a>; but at least in the sciences a Ph.D. can take 4 years. As post-doctoral positions in the humanities multiply, it pushes the paper qualifications of new applicants higher and higher (and their ages older and older). The ABD hire is rapidly disappearing; some entry-level jobs now require a Ph.D. 9 months <em>before</em> the start date. For this to happen, there must be some large store of accredited individuals in a holding pen somewhere. And increasingly, those pens are post-doctoral programs.</p>
<p>Insofar as the extra time lets hiring committees better distinguish between applicants, it’s a fine thing. But it also creates perverse effects. First and foremost, it makes the tenure track job market more and more into a waiting game; the interplay between pigeon-holed job descriptions and endless benches to warm on create a system where many faculty can <em>eventually</em> get a job if they don’t let their resumes get too clogged by things that aren’t teaching and publishing.</p>
<p>This is called rationing by queue. It’s familiar from Wal-Marts before Thanksgiving and Disneyworld after 8:30am, but the most famous example are the post-Soviet bread lines. The prices for bread are not high enough to make the poorest starve, so scarce bread goes those who are willing to spend the most time waiting for the bakery to be open or the shipment to arrive:</p>
<p><a href="http://4.bp.blogspot.com/-aMi6f72D2qg/Tt5W_7QHsOI/AAAAAAAAC6w/GyZd7XXeS4c/s1600/Latvian+Bread+line.jpg"><img src="http://4.bp.blogspot.com/-aMi6f72D2qg/Tt5W_7QHsOI/AAAAAAAAC6w/GyZd7XXeS4c/s400/Latvian+Bread+line.jpg" /></a></p>
<p>Fig. 1: Outside the Chicago Sheraton, Jan. 7, 2012</p>
<p>Now, rationing by queue can be a good thing if we want to protect a commodity from market pressures. Disneyworld would be less magical if you had to pay a variable rate for each ride; liver transplants are less just if the wealthiest can <a href="http://www.ama-assn.org/amednews/2009/07/27/prsa0727.htm">buy themselves to the head of the line</a>. The textbook economic solution to the oversupply of Ph.D.s would be to let the benefits associated with a tenured job (pay, security, workload, respect) plunge until finally everyone just gave up. To its credit historians have not let this happen; but the continuing attack on all of those perks from administrators, politicians, etc., is a direct result of those market forces pushing on the profession. Rationing by queue isn’t working at stopping market forces, and in the meantime we’re rewarding waiting too strongly.</p>
<p>Adding a new postdoctoral position is not baking more bread; it’s buying concrete to make the sidewalk longer. True, more people can stand in line. But that just makes the situation worse for the next <em>babushka</em> who wants a sandwich; anyone who has better things to do than spend nine years standing in line is going to give up. And we’re out the money for a sidewalk.</p>
<p>~~~</p>
<p>So OK: we eliminate the post-docs. Great for the long-term prospects of the profession. But short term, all we’re doing is throwing a bunch of people onto the street. Some of them will end up in adjuncting jobs (structurally similar to post-doctoral positions, though less often defended), and some will be out of luck. But remember: this is about using our resources: and taking all that post-doc money plus some more, we can do good, not harm.</p>
<p><strong>Step 2:</strong> <strong>Using money from graduate and postdoctoral fellowships, initiate a massive program of buy-outs at different professional stages.</strong></p>
<p>Instead of spending what little money we have to make the tenure-track market <em>worse</em>, why not use it to make it better?</p>
<p>A program of buy-outs would serve many of the same purposes as contraction, but would make the decision to leave a choice of the individual, and sidestep the problem of which departments would have to shrink. Those departments not training for the tenure track would be relatively unaffected; those that are would be only indirectly affected. Unlike contraction, it would immediately relieve the crush on the job market.</p>
<p>Here’s how it would work. Postdoctoral fellowships review committees would stay as they are, and would review applications with statements and letters of recommendation. But instead of paying you to come apply to jobs, they would pay you more (since it wouldn’t include overhead like office space or health care) to simply check out of the profession. Instead of a $30,000/year, plus another $10K (say) building and health care overhead for two years, they would pay $65,000 up front for three things:</p>
<ol type="1">
<li><p>A firm commitment to never seek tenure-track employment in a history program.</p></li>
<li><p>A commitment from each of your committee members and department chair to not send letters of recommendation on your behalf to tenure-track jobs. They would be compensated for this as well–probably around $5K for the primary advisor. The point would be ensure compliance, and to promote conversations between students and advisors about whether they should take the option. (They also get all the departmental service points, if such things exist, they would have received for advising the dissertation to completion).</p></li>
<li><p>All of your dissertation-related intellectual property. It would be placed into a public domain (no attribution necessary) repository managed by the AHA. Archival photographs, out-of-copyright scans, notes and completed, unpublished chapters—it all goes into the digital commons. This a) makes it harder for you to welch on your commitment to leave academia; b) builds up a great base for open materials for future teaching, research, dissertation proposals, etc; and c) gives the selection committees a good incentive to choose well in buying people out. (This one might be a harder sell, so we could make it only possible on a sliding scale).</p></li>
</ol>
<p>Ph.D.s would not be required, but the point would be to pull people off the market who <em>look like prime candidates for tenure track jobs</em>; that’s how these reviews would work.</p>
<p>If you are a currently enrolled grad student, you could apply for one of the buy-out fellowships; but you could also convert the remaining cash value of your stipend anytime after reaching ABD status. So if you had 2 years @ $25K with six course sections, say, you could deduct the marginal cost for a TA (call it 12K?), add 33% for health care and other overhead, and walk out with $50,500 and a master’s degree. Once again, some money–5%?–would go to advisors. This would strongly increase everyone’s incentives to talk clearly and openly about their academic prospects after reaching candidacy; just the point that many students, now, see no alternative but staying the course.</p>
<p>We might be able to open it up to junior professors, as well; nothing helps the job market as much as flushing out the people actually in the jobs. We’ll have to get some econometricians on board to set the rates properly for them (which will rely mostly on their tenure prospects). Senior faculty, on the other hand, we’ll leave out; universities already have a great incentive to force them into retirement and replace them with younger, cheaper replacements.</p>
<p>The reserve army of the unemployed used as contingent faculty will <em>not</em> be eligible. But since many people who would have entered that pool will have been bought out, the buyouts will help them all immediately. Raising the market-clearing rate for qualified adjuncts will improve conditions for those who remain; and as their price goes up, in marginal cases it will allow departments to make stronger cases for new tenure-track lines. Bought-out Ph.D.s could still teach in adjunct positions; in many cases, they might find it rewarding personally or a way to tide over a rough financial period. But with the brass ring permanently revoked, many fewer would choose to.</p>
<p>Objections:<br />
<strong>1. This would lead to lots of people attending graduate school for two to three years, then pocketing the buyout money and leaving.</strong><br />
Great! We’ve just achieved some of the benefits of contraction while still exposing more students to the possibilities of academic history, and showed the other students and faculty in those programs alternatives to the 7-year forced march towards tenure.</p>
<p><strong>2. The buyouts could take the best historians out of the pool, leaving us with a generation of assistant professors whose chief virtue is that they were unable to conceive of doing anything else with their lives.</strong><br />
Maybe: but that’s exactly what queue-rationing is doing right now. We already lose all people who want to save for their retirement in their 20s, live with their spouse every year of their 30s, or live in the same metro area as their aging parents in their 40s. If the goal is to get the best professors, our first priority needs to be change the compensation scheme to <em>make those things possible,</em> not to reinforce the status quo. In the short run, things might get a bit worse. But in the long run, we’ll have a better chance of attracting and keeping the best people possible.</p>
<p><strong>3. There’s not enough money in the system to make this work.</strong><br />
We’ll have to see. At the very least, we’ll have the benefit of shutting down the post-docs. If it doesn’t prove enough to make the market easier, than maybe we start putting some of that money towards internships or something instead. But it’s not clear that there’s money in the federal government for a new WPA, or in the broader society for collaborative public history jobs, either.</p>
<p><strong>4. This is a neo-liberal call to submit to the market.</strong><br />
To the contrary: this is a call to take control of the commanding heights of the market. The surest way to let market forces (adjunctification, falling salaries and number of tenure slots, etc.) decimate the profession is willful blindness to its operations; that has been the preferred choice of most humanists for the last thirty years. Why not try something new?</p>
<hr />
<h3 id="comments">Comments:</h3>
<h4 id="i-cant-decide-whether-or-not-this-is-an-ironi">I can’t decide whether or not this is an ironi…</h4>
<p><a href="#">Anonymous</a> - <time datetime="2012-01-30T19:44:15.549-05:00">Jan 1, 2012</time></p>
<p>I can’t decide whether or not this is an ironic “Modest Proposal.” The word “modest” in the title makes me suspicious.</p>
<p>In any case, I like it. You are *so* right about “rationing by queue.” Personally, though, I remain an old-fashioned contractionist.</p>
<hr />
<h4 id="yeah-me-neither.-i-think-stanley-fish-has-left-us">Yeah, me neither. I think Stanley Fish has left us…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-01-30T20:07:01.827-05:00">Jan 1, 2012</time></p>
<p>Yeah, me neither. I think Stanley Fish has left us all on edge about authorial intent.</p>
<hr />
<h4 id="this-is-ben-pasting-in-with-permission-an-anony">[This is Ben pasting in, with permission, an anony…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-02-01T10:46:05.656-05:00">Feb 3, 2012</time></p>
<p>[This is Ben pasting in, with permission, an anonymous comment received over e-mail]</p>
<p>Provocative stuff as always, Ben. Though I’m not sure you are targeting the right part of the system by looking generically at all “postdocs.” (Obvious disclaimer that I have recently been applying to a lot of postdocs etc etc.)</p>
<p>How much queue-rationing is really going on via the postdocs? I’m not as convinced as you are that there is such a misalignment between the number of people who can get postdocs and the number of people who can get tenure-track jobs. Many postdoc competitions are exceedingly competitive–600, 800, 1,000 applicants for 3-6 spots, at the worst–and it seems that, for many people, the barrier between Ph.D. and postdoc is just as daunting as that between post-doc and job.</p>
<p>So how many people are really getting into that “holding pen,” and how many of those who do end up not finding longer term positions? I’d love to see some kind of analysis of just how many people the “humanities postdoc holding pen” can really support, because if I had to guess I would suspect that the number is like well below 50% of all people completing Ph.Ds.</p>
<p>Plus, a lot of postdocs are interdisciplinary, open-call competitions that tend to value work that is methodologically innovative, that speaks to themes of relevance across a variety of fields, that attends to questions that have been overlooked by the academy, etc. This would seem to me to actually do a good job of keeping people in the game who are likely to be successful later on. And, indeed, people who get the good humanities postdocs (the “Societys,” Mellons, Humanities Centers, etc.) seem to have a very good track record of getting tenure-track jobs.</p>
<p>So don’t you really mean that we should be making distinctions between different kinds of postdocs: between those that pay glorified grad stipends and those that pay (something closer to) a meaningful salary; between those that require 3/3 teaching loads (miscoded adjuncting) and those that offer modest/no teaching loads and considerable research support; between those that are limited, apprenticeship positions to work on senior scholars’ projects (“development of accounting practices in the antebellum United States”) and those that offer a kind of research flexibility that many scholars covet once they get tenure-track jobs?</p>
<p>So I just don’t see how getting rid of the good humanities postdocs really helps anybody. In which case, your proposal would seem to be more targeted at the various less attractive postdocs (those that are basically VAP positions; those that are one-offs and don’t have particular market-signalling power; those that are tied to someone elses research project). And I guess I’m on board with you on that point. But then the question is, how big is that pool, really?</p>
<p>If there is enough money to be found by taking it away from postdocs, it would have to come from taking it away from mostly the big, well-funded postdoc programs (read: appropriating all of Andrew Mellon’s money), which would seem to be doing something kind of useful…</p>
<hr />
<h4 id="anon-i-admit-to-not-having-a-clue-about-the-rela"><span class="citation" data-cites="anon">@anon</span>: I admit to not having a clue about the rela…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2012-02-01T12:35:54.747-05:00">Feb 3, 2012</time></p>
<p><span class="citation" data-cites="anon">@anon</span>: I admit to not having a clue about the relative numbers of ‘bad’ and ‘good’ postdocs. As you say, it’s frequently hard to tell ‘bad’ postdocs and adjuncts apart since both take similar advantage of a saturated market to get teaching/research work at rates lower than we’d all like PhDs to be paid. It would certainly be great to get rid of both of those, but we’d lose a lot of labor that couldn’t be replaced as cheaply.</p>
<p>As for overall numbers, I don’t have them. I suspect a lot of the benefit from this proposal would come not from the provocative post-doc part, but from the ABD buyouts and other systems to push people out of the field. (More realistically, I love the <a href="http://www.acls.org/programs/publicfellows/">ACLS public fellows program</a> along these lines.) But if the numbers are still insignificant (<a href="http://www.mla.org/bulletin_124043">as they were a decade ago in English</a>) my primary goal is to say <strong>we should avoid at all costs the science model of near-universal postdocs</strong>. But we’re well down that road–<a href="http://www.nsf.gov/statistics/sed/data_table.cfm">about a third as many</a> history doctorate recipients report some form of ‘postdoctoral study’ as their plan on receiving the Ph.D. right now as report employment of any kind in the academic sector. (I can’t immediately find the Ph.D. breakdowns).</p>
<p>That means, more substantively, that I actually want to focus <em>first</em> on eliminating the big, prestigious postdocs of the last two decades. I’d argue the positives and less, and the harms greater, than you claim.</p>
<p>The key question is: who do they benefit? You say it preserves people who will do well ‘later on.’ Insofar as that’s true, though, I think it also helps people who do well right now: methodologically innovative, relevant work is the easiest stuff to sell in the current job market. (What more likely would need to be tided over is obscure stuff that might not find a possible opening in any given year). And this is one feature of postdocs I don’t mention: frequently, they go to people who get jobs right away or who already have jobs. This is great for them, but systematically means we’re at best reinforcing the winner-gets-all aspects of the current tenure system. Spending hundreds of thousands or low millions a year on helping the people who are doing the best in the profession is a crazy use of funds. (In Claire Potter’s honor, I’ll go so far as to call it “neo-liberal.”) Rather than give them an extra year of research, I think we should give them an extra year of university life by allowing them to get jobs ABD as they did in the past.</p>
<p>But I think it’s worse than just a dismal use of money. We’re in fact encouraging those who want tenure to spend yet another year schlepping around the country. (The more people take up post-docs, the more reasonable a two-book tenure standard for tenure comes to seem, for example–so even if you’d rather get straight on the tenure track, that comes to be an increasingly bad idea if you can get a research fellowship.) Encouraging this promotes the loss of people who aren’t willing to take on the burden of another move, and leads people to discourage promising students from going to grad school. Nor is it an evenly distributed burden: requiring another move falls particularly harshly on those with non-portable spouses and for people with primary child-care obligations, which raises gender equity issues pretty strongly.</p>
<p>One thing you might have to spell out a bit more: I don’t see any contradiction between good post-docs being hard to get, and them being a form of queue-rationing. If they simply allow the best people to polish up their work before going on the market, that’s the very definition of queue-rationing.</p>
<hr />
<h4 id="will-the-buyout-include-a-selection-of-delicious-b">Will the buyout include a selection of delicious b…</h4>
<p><a href="">Anonymous</a> - <time datetime="2012-04-09T00:56:37.916-04:00">Apr 1, 2012</time></p>
<p>Will the buyout include a selection of delicious babies? Sounds like a great idea!</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Corpuses by Word Use]]></title>
        <id/>
        <link/>
        <updated>2011-10-06T19:36:00.000Z</updated>
        <content type="html"><![CDATA[<p>Historians often hope that digitized texts will enable better, faster comparisons of groups of texts. Now that at least the 1grams on <a href="http://bookworm.culturomics.org/">Bookworm</a> are running pretty smoothly, I want to start to lay the groundwork for using corpus comparisons to look at words in a big digital library. For the algorithmically minded: this post should act as a somewhat idiosyncratic approach to Dunning’s Log-likelihood statistic. For the hermeneutically minded: this post should explain why you might need _any_ log-likelihood statistic.</p>
<p>What are some interesting, large corpuses to compare? A lot of what we’ll be interested in historically are subtle differences between closely related sets, so a good start might be the two Library of Congress subject classifications called “History of the Americas,” letters E and F. The Bookworm database has over 20,000 books from each group. What’s the difference between the two? The full descriptions could tell us: but as a test case, it should be informative to use only the texts themselves to see the difference.</p>
<p>That leads a tricky question. Just what does it mean to compare usage frequencies across two corpuses? This is important, so let me take this quite slowly. (Feel free to skip down to Dunning if you just want the best answer I’ve got.) I’m comparing E and F: suppose I say my goal to answer this question:</p>
<p><strong>What words appear the most times more in E than in F, and vice versa?</strong></p>
<p>There’s already an ambiguity here: what does “times more” mean? In plain English, this can mean two completely different things. Say E and F are exactly the same overall length (eg, each have 10,000 books of 100,000 words). Suppose further “presbygational” (to take a nice, rare, American history word) appears 6 times in E and 12 times in F. Do we want to say that it appears two times more (ie, use multiplication), or six more times (use addition)?</p>
<p>It turns out that neither of these simple operations works all that well. In the abstract, multiplication probably sounds more appealing; but it turns out to only catch extremely rare words. In our example set, here are the top words that distinguish E from F by multiplication, by occurences in E divided by occurrences in F. For example, “gradualism” appears 61x more often in E than in F.</p>
<p>daimyo        aftre    exercitum intransitive      castris        1994a<br />
   114.00000    103.00000    101.00000     82.33333     81.66667     77.00000<br />
       sherd        infti   gradualism  imperforate      equitum       brynge<br />
    71.71429     66.00000     61.00000     59.33333     57.00000     56.00000</p>
<p>(BWT, I simply omit the hundreds of words that appear in E but never appear in F; and I don’t use capitalized words because they tend to _very_ highly concentrated and in fictional works in particular can cause very strange results. Yes, that’s not the best excuse.)</p>
<p>So what about addition? Compensating for different corpus sizes, it’s also pretty easy to find out the number of more occurrences than we’d expect based on the previous corpus. (For example, “not” appears about 1.4 million more times than we’d expect in E given the number of times it appears in F and the total number of words in E.)</p>
<p>to      that       the       not       had        it<br />
3432895.3 2666614.4 2093465.8 1427220.8 1360559.0 1342948.2<br />
       be   general        we       but       our     would<br />
1208340.5  990988.4  974849.0  841842.6  819680.5  798426.0</p>
<p>Clearly, neither of these is working all that well. Basically, the first group are so rare they don’t tell us much: and the second group, with the intriguing addition of “general”, are so common as to be uninformative. (Except maybe for “our”; more on that later). Is there any way to find words that are interesting on _both_ counts?</p>
<p>I find it helpful to do this visually. Suppose we make a graph. We’ll put the addition score on the X axis, and the multiplication one on the Y axis, and make them both on a logarithmic scale. Every dot represents a word, as it scores on both of these things. Where do the words we’re talking about fall?</p>
<p><a href="http://1.bp.blogspot.com/-gjVv_rD25JE/TozQTWhVfMI/AAAAAAAAC3Q/gZHjE9MFICE/s1600/multiplication+addition+comparison.png"><img src="http://1.bp.blogspot.com/-gjVv_rD25JE/TozQTWhVfMI/AAAAAAAAC3Q/gZHjE9MFICE/s640/multiplication+addition+comparison.png" /></a></p>
<p>This nicely captures our dilemma. The two groups are in opposite corners, and words don’t ever score highly on both. For example, “general” appears about 1,000,000 occurrences more in class E than we’d expect from class F, but only about 1.8x as often; <a href="http://en.wikipedia.org/wiki/Sherd">sherd</a> appears about 60x more often in class E, but that adds up to only 500 extra words compared to expectations, since it’s a much rarer word overall.</p>
<p>(BTW, log-scatter plots are fun. Those radiating spots and lines on the left side have to do with the discreteness of our set; a word can appear 1 time in a corpus or twice in a corpus, but it can’t appear 1.5 times. So the lefternmost line is words that appear just once in F: the single point farthest left, at about (1.1,2.0) is words that appear twice in E and once in F; a little above it to the right is words that appear three times in E and once in F; directly to its right are words that appear four times in E and twice in F; etc.)</p>
<p>One possible solution would be to simply draw a line between “daimyo” and “that”, and assume that words are interesting to the degree that they stick out beyond that line. That gives us the following word list, placed on that same chart:</p>
<p><a href="http://2.bp.blogspot.com/-vNTk3GURBk0/Toz1kEFrMTI/AAAAAAAAC3U/qocFwwOT-5w/s1600/Ben%2527s+Log+Likelihood.png"><img src="http://2.bp.blogspot.com/-vNTk3GURBk0/Toz1kEFrMTI/AAAAAAAAC3U/qocFwwOT-5w/s640/Ben%2527s+Log+Likelihood.png" /></a></p>
<p><a href="http://1.bp.blogspot.com/-gjVv_rD25JE/TozQTWhVfMI/AAAAAAAAC3Q/gZHjE9MFICE/s1600/multiplication+addition+comparison.png"><br />
</a></p>
<p>…which is a lot better. The words are specific enough to be useful, but common enough to be mostly recognizable. Still, though, the less frequent words seem less helpful. Are “sherd” and “peyote” and “daimyo” up there because they really characterize the difference between E and F, or because a few authors just happened to use them a lot? And why assume that “that” and “daimyo” are equally interesting? Maybe “that” actually _is_ more distinctive than daimyo, or vice-versa.</p>
<p>To put it more formally: words to the left tend to be rarer (for a word to have 100,000 more occurrences than we’d expect, it has to be quite common to begin with); and there are a lot more rare words than common words. So by random chance, we’d expect to have more outliers on the top of the graph than on the bottom. <a href="http://bookworm.culturomics.org/?%7B%22query%22%3A%7B%22index%22%3A0%2C%22time_measure%22%3A%22year%22%2C%22time_limits%22%3A%5B1815%2C1922%5D%2C%22counttype%22%3A%22Percentage_of_Books%22%2C%22words_collation%22%3A%22Case_Sensitive%22%2C%22smoothingSpan%22%3A%225%22%2C%22search_limits%22%3A%5B%7B%22word%22%3A%5B%22daimyo%22%5D%2C%22lc0%22%3A%5B%22E%22%5D%7D%2C%7B%22word%22%3A%5B%22daimyo%22%5D%2C%22lc0%22%3A%5B%22F%22%5D%7D%5D%7D%2C%22terms%22%3A%5B%22daimyo%22%5D%2C%22category_data%22%3A%5B%5B%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22country%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22E%22%5D%5D%5D%2C%5B%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22country%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22F%22%5D%5D%5D%5D%2C%22comparison%22%3A%22texts%22%7D">By using Bookworm to explore the actual texts</a>, I can see that “<a href="http://daimyo/">daimyo</a>” appears so often in large part because Open Library doesn’t recognize these <a href="http://openlibrary.org/books/OL13516236M/The_early_diplomatic_relations_between_the_United_States_and_Japan_1853-1865">two</a> <a href="http://openlibrary.org/books/OL7047991M/The_early_diplomatic_relations_between_the_United_States_and_Japan_1853-1865">books</a> are the same work. Conversely, that “our” appears 20% more often in E than in F is quite significant; looking at the chart, it <a href="http://bookworm.culturomics.org/?%7B%22query%22%3A%7B%22index%22%3A0%2C%22time_measure%22%3A%22year%22%2C%22time_limits%22%3A%5B1815%2C1919%5D%2C%22counttype%22%3A%22Occurrences_per_Million_Words%22%2C%22words_collation%22%3A%22Case_Insensitive%22%2C%22smoothingSpan%22%3A%220%22%2C%22search_limits%22%3A%5B%7B%22word%22%3A%5B%22our%22%5D%2C%22lc0%22%3A%5B%22E%22%5D%7D%2C%7B%22word%22%3A%5B%22our%22%5D%2C%22lc0%22%3A%5B%22F%22%5D%7D%5D%7D%2C%22terms%22%3A%5B%22our%22%5D%2C%22category_data%22%3A%5B%5B%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22country%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22E%22%5D%5D%5D%2C%5B%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22country%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22F%22%5D%5D%5D%5D%2C%22comparison%22%3A%22texts%22%7D">seems to actually hold true</a> across a long period time. If this is a problem with 20,000 books in each set, it will be far worse when we’re using smaller sets.</p>
<p>That would suggest we want a method that takes into account the possibility of random fluctuations for rarer ones. One way to do this is a technique called, after its inventor, <a href="http://wordhoard.northwestern.edu/userman/analysis-comparewords.html">Dunning’s log-likelihood statistic</a>. I won’t explain the details, except to say that like our charts it uses logarithms and that it is much more closely to our addition measure than to the multiplication one. On our E vs F comparison, it turns up the following word-positions (in green) as the 100 most significantly higher in E than F:</p>
<p><a href="http://3.bp.blogspot.com/-v3IlGm7fXCk/Toz9qIkckmI/AAAAAAAAC3Y/8p7Htulf6GU/s1600/Dunning+Log-likelihood+demonstration.png"><img src="http://3.bp.blogspot.com/-v3IlGm7fXCk/Toz9qIkckmI/AAAAAAAAC3Y/8p7Htulf6GU/s1600/Dunning+Log-likelihood+demonstration.png" /></a></p>
<p>Dunning’s log-likelihood uses probabilistic statistics to approximate a chi-square test; as a result, the words it identifies tend to come from the most additively over-represented, but it also gives some credit for multiplication. All of the common words from our initial sets of 12 additive words, and none of the rare ones, are included. It includes about half of the words my naive straight-line method produced: even “skirmisher”, which seemed to clump with the more common words, isn’t frequent enough for Dunning to privilege it over a blander word like “movement”.</p>
<p>Is this satisfying? I should maybe dwell on this longer, because it really matters. Dunning’s is the method that seems to be most frequently used by digital humanities types, but the innards aren’t exactly what you might think. In MONK, for example, the words with the highest Dunning scores are represented as bigger, which may lead users to think Dunning gives a simple frequency count. It’s not–it’s fundamentally a probability measure. We can represent it like it has to do with frequency, but it’s important to remember that it’s not. (Whence the curve on our plot).</p>
<p>Ultimately, what’s useful is defined by results. And I think that the strong showing of common words can be quite interesting. This ties back to my point a few months ago that <a href="http://sappingattention.blogspot.com/2011/04/stopwords-to-wise.html">stopwords carry a lot of meaning in the aggregate</a>. If I didn’t actually really find the stopwords useful, I’d be more inclined to put some serious effort into building my own log-difference comparison like the straight line above; as it is, I’m curious if anyone knows of some good ones.</p>
<p>As for results, here’s what Dunning’s test turns up in series E and in series F, limiting ourselves to uncapitalized words among the 200,000 most common in English:</p>
<p><strong>Significantly overrepresented in E, in order**</strong>:**</p>
<p>[1] “that” “general” “army” “enemy”</p>
<p>[5] “not” “slavery” “to” “you”</p>
<p>[9] “corps” “brigade” “had” “troops”</p>
<p>[13] “would” “our” “we” “men”</p>
<p>[17] “war” “be” “command” “if”</p>
<p>[21] “slave” “right” “it” “my”</p>
<p>[25] “could” “constitution” “force” “what”</p>
<p>[29] “wounded” “artillery” “division” “government”</p>
<p><strong>Significantly overrepresented in F, in order:</strong></p>
<p>[1] “county” “born” “married” “township”</p>
<p>[5] “town” “years” “children” “wife”</p>
<p>[9] “daughter” “son” “acres” “farm”</p>
<p>[13] “business” “in” “school” “is”</p>
<p>[17] “and” “building” “he” “died”</p>
<p>[21] “year” “has” “family” “father”</p>
<p>[25] “located” “parents” “land” “native”</p>
<p>[29] “built” “mill” “city” “member”</p>
<p>At a first pass, that looks like local history versus military history.</p>
<p>At a second pass, we’ll notice ‘constitution’ and ‘government’ in F and ‘he’ and ‘parents’ in E and realize that E might include biographies as well as local histories, and that F probably includes a lot of legal and other forms of national histories as well. The national words might not have turned up by my straight-line test, which seemed intent on finding all sorts of rarer military words (“skirmishers”, for example).</p>
<p>Looking at the official LC classification definition (<a href="http://www.loc.gov/aba/cataloging/classification/lcco/lcco_ef.pdf">pdf</a>), that turns out to be mostly be the case. (Except for biography–that ought to mostly be in E. That it isn’t is actually quite interesting.) So this is reasonably good at giving us a sense of the differences between corpuses as objectively defined. So far, so good.</p>
<p>But these lists are a) not engaging, and b) don’t use frequency data. How can we fix that? I never thought I’d say this, but: let’s <a href="http://www.wordle.net/">wordle</a>! Wordle in general is a heavily overrated form of text analysis; Drew Conway has a nice post from a few months ago criticizing it because it doesn’t <a href="http://www.drewconway.com/zia/?p=2624">use a meaningful baseline of comparison, and uses spatial arrangement arbitrarily.</a> Still, it’s super-engaging, and possibly useful. We can make use of the Dunning data here to solve the first problem though not the second. Unlike in a normal Wordle, where size is frequency, here size is Dunning score: and the word clouds are <em>paired</em>, so each one represents two ends of a comparison. Here’s a graphic representing class E:</p>
<p><a href="http://3.bp.blogspot.com/-bFq3kc1NGzs/To3UjOE22EI/AAAAAAAAC3c/4bqdPNLxmFA/s1600/LC+Classificatin+E+words.png"><img src="http://3.bp.blogspot.com/-bFq3kc1NGzs/To3UjOE22EI/AAAAAAAAC3c/4bqdPNLxmFA/s640/LC+Classificatin+E+words.png" /> </a></p>
<p>And then Class F:</p>
<p><a href="http://3.bp.blogspot.com/-Y6nQnDqdq8s/To3Ujui8YHI/AAAAAAAAC3g/jMPaYa5_ggA/s1600/Class+F+Distinguishing+words.png"><img src="http://3.bp.blogspot.com/-Y6nQnDqdq8s/To3Ujui8YHI/AAAAAAAAC3g/jMPaYa5_ggA/s640/Class+F+Distinguishing+words.png" /></a></p>
<p>(We could also put them together and color-code like MONK does, but I think it’s easier to get the categories straight by splitting them apart like this). One nice thing about this is that the statistical overrepresentation of ‘county’ in class F really comes through.</p>
<p>On some level, this is going to seem unremarkable–we’re just confirming that the LC description does, in fact, apply. But a lot of interesting thoughts can come from the unlikely events in here. For example, ‘our’ and ‘we’ are both substantially overrepresented in the national histories as opposed to the local histories. (BTW, I should note somewhere that both E and F include a fair number of historical _documents_, speeches, etc., as well as histories themselves. Here, I’m lumping them all together.) There’s no reason this should be so–local histories are often the most intensely insular.</p>
<p>Is there a historical pattern in the second-person-plural? <a href="http://bookworm.culturomics.org/?%7B%22query%22%3A%7B%22index%22%3A0%2C%22time_measure%22%3A%22year%22%2C%22time_limits%22%3A%5B1815%2C1922%5D%2C%22counttype%22%3A%22Occurrences_per_Million_Words%22%2C%22words_collation%22%3A%22Case_Sensitive%22%2C%22smoothingSpan%22%3A%225%22%2C%22search_limits%22%3A%5B%7B%22word%22%3A%5B%22we%22%2C%22our%22%5D%2C%22lc0%22%3A%5B%22E%22%5D%7D%2C%7B%22word%22%3A%5B%22we%22%2C%22our%22%5D%2C%22lc0%22%3A%5B%22F%22%5D%7D%5D%7D%2C%22terms%22%3A%5B%22we%2Cour%22%5D%2C%22category_data%22%3A%5B%5B%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22country%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22E%22%5D%5D%5D%2C%5B%5B%22state%22%2C%5B%5D%5D%2C%5B%22lc1%22%2C%5B%5D%5D%2C%5B%22country%22%2C%5B%5D%5D%2C%5B%22lc0%22%2C%5B%22F%22%5D%5D%5D%5D%2C%22comparison%22%3A%22texts%22%7D">Bookworm says yes, emphatically</a>--in a quite interesting way. “We’s” and “Us’s” are similar across E and F in the early republican period, and then undergo some serious wiggling starting around the Civil War; that leads to a new equilibrium around 1880 with E around its previous height, and F substantially lower.</p>
<p>Now, there doesn’t <em>have</em> to be an interesting historical explanation for this. Maybe it’s just about memoirs switching from F to E, say. But there might be: we could use this sort of data as a jumping off point for some explorations of nation-building and sectionalism. For example, clicking on the E results around 1900 gives books that use the words ‘we’ and ‘our’ the most. One thing I find particularly interesting there are the presence of many books that, by the titles at least, I’d categorize as African-American racial uplift literature. (That’s a historian’s category, of course, not a librarian’s one). If we were to generalize that, it might suggest the rise of several forms of authorial identification with national communities (class, race, international, industrial) in the late nineteenth century, and a corresponding tendency to <em>not</em> necessarily see local history as first-person history. Once we start to investigate the mechanics of <em>that,</em> we can get into some quite sophisticated historical questions about the relative priority of different movements in constructing group identities, connections between regionalism in the 1850s vs. (Northern?) nationalism in the 1860s, etc.</p>
<p>We aren’t restricted to questions where the genres are predefined by the Library of Congress. There is a _lot_ to do with cross corpus comparisons in a library as large as the Internet Archive collection. We can compare authors, for example: I’ll post that bit tomorrow.</p>
<p>This isn’t stuff that Martin and I could integrate into Bookworm right away, unfortunately. It simply takes too long. The database driving Bookworm can add up the counts for any individual word in about half a second; it takes more like two minutes to add up all the words in a given set of books. For a researcher, that’s no time at all; but for a website, it’s an eternity. Both from the user end (no one will wait that long for data to load) and from the server end (we can’t handle too many concurrent queries, and longer queries means more concurrent ones).</p>
<h2 id="but-wordle-clouds-and-ui-issues-aside-the-base-idea-has-all-sorts-of-applications-ill-get-into-more-later.">But Wordle clouds and UI issues aside, the base idea has all sorts of applications I’ll get into more later.</h2>
<h3 id="comments">Comments:</h3>
<h4 id="this-is-brilliant-stuff.-ive-been-taking-the">This is brilliant stuff. I’ve been taking the …</h4>
<p><a href="#">Anonymous</a> - <time datetime="2011-10-07T22:32:05.282-04:00">Oct 5, 2011</time></p>
<p>This is brilliant stuff. I’ve been taking the value of Dunning’s more or less on faith. By inspecting the formula I could see that it struck some kind of compromise between the measures of difference you’re characterizing as “multiplicative” and “additive,” but graphing that as a log scatterplot is a fabulous idea. It makes visible exactly what kind of compromise the formula in practice strikes.</p>
<p>I’m very much of the opinion that, as you say, “what’s useful is defined by results,” and I’ve sometimes felt that the Dunnings results I get from MONK emphasize stopwords more than I want (perhaps just because I’m not good at interpreting them). I might be a tiny bit tempted to tinker with the Dunning’s algorithm by varying the base that gets used for the log function. It’s supposed to be log base e, but by fiddling with the base I bet you could stretch the “green” region a bit toward the y axis …</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Call numbers]]></title>
        <id/>
        <link/>
        <updated>2010-12-27T18:45:00.000Z</updated>
        <content type="html"><![CDATA[<p>I finally got some call numbers. Not for everything, but for a better portion than I thought I would: about 7,600 records, or c. 30% of my books.</p>
<p>The <a href="http://www.hathitrust.org/bib_api">HathiTrust Bibliographic API</a> is <em>great.</em> What a resource. There are a few odd tricks I had to put in to account for their integrating various catalogs together (Michigan call numbers are filed under MARC 050 (Library of Congress catalog), while California ones are filed under MARC 090 (local catalog), for instance, although they both seem to be basically an LCC scheme). But the openness is fantastic–you just plug in OCLC or LCCN identifiers into a url string to get an xml record. It’s possible to get a lot of OCLCs, in particular, by scraping Internet Archive pages. I haven’t yet found a good way to go the opposite direction, though: from a large number of specially chosen Hathi catalogue items to IA books.</p>
<p>This lets me get a slightly better grasp on what I have. First, a list of how many books I have for each headline LC letter:</p>
<p>A    B    C    D    E    F    G<br />
  83  693   65 1076  714  323  126<br />
   H    J    K    L    M    N    O    P<br />
 401  171   35  271   37  138    1 2679<br />
   Q    R    S    T    U    V    Z<br />
 418   86  108  142   29   16   62</p>
<p>P is literature and criticism, which there’s a lot of. C, D, E and F are history, the latter two American; B is philosophy and religion, H is social sciences, Q is physical sciences, L is education, and nothing else (medicine, technology, etc—even “O,” which I don’t think exists) is over 200 books.</p>
<p>So what do we do with them? First, let’s make sure they look like they make sense on some old problems. How much do these classes use the word “evolution”?</p>
<p>Description Permille<br />
Q                             SCIENCE    0.163<br />
B    PHILOSOPHY. PSYCHOLOGY. RELIGION    0.108<br />
H                     SOCIAL SCIENCES    0.079<br />
L                           EDUCATION    0.078<br />
M            MUSIC AND BOOKS ON MUSIC    0.077<br />
O                                 0.045<br />
R                            MEDICINE    0.041<br />
U                    MILITARY SCIENCE    0.037<br />
N                           FINE ARTS    0.035<br />
Z      BIBLIOGRAPHY. LIBRARY SCIENCE.    0.034<br />
T                          TECHNOLOGY    0.034<br />
                                    0.031<br />
J                   POLITICAL SCIENCE    0.030<br />
V                       NAVAL SCIENCE    0.024<br />
S                         AGRICULTURE    0.021<br />
A                       GENERAL WORKS    0.020<br />
C       AUXILIARY SCIENCES OF HISTORY    0.016<br />
G GEOGRAPHY. ANTHROPOLOGY. RECREATION    0.016<br />
E             HISTORY OF THE AMERICAS    0.009<br />
P             LANGUAGE AND LITERATURE    0.009<br />
F             HISTORY OF THE AMERICAS    0.008<br />
D                       WORLD HISTORY    0.008<br />
K                                 LAW    0.003</p>
<p>Looks about right, although I wouldn’t have thought history or particularly anthropology would be quite so low. The numbers mean that, for instance, 0.16 out of every thousand words in science books are the word “evolution.” I can list the highest density subclasses:</p>
<p>&gt; class.summary(“evolution”)[1:10,]<br />
working on item number 1 of 1 (evolution)…<br />
                         Description Permille<br />
QH         Natural history - Biology    0.783<br />
HM               Sociology (General)    0.533<br />
JA       Political science (General)    0.506<br />
RB                         Pathology    0.329<br />
BL Religions. Mythology. Rationalism    0.316<br />
B   PHILOSOPHY. PSYCHOLOGY. RELIGION    0.261<br />
BD            Speculative philosophy    0.238<br />
RZ         Other systems of medicine    0.214<br />
BJ                            Ethics    0.201<br />
QD                         Chemistry    0.188</p>
<p>Some of these classes are quite small, but the general numbers look about right. Sociology uses the word “evolution” almost a hundred times more frequently than American history. That’s a nice illustration of, though not a revelation about, the importance of evolutionary thought for some of the social sciences even compared to biology. Ethics is higher than I would have thought—I’ll have to correct my sampling functions so I can get text examples of just what that is.</p>
<p>How about more common words? Like the one I’m writing about?</p>
<p>&gt; class.summary(“attention”,shorten=T)[1:10,]<br />
                       Description Permille<br />
L                        EDUCATION    0.429<br />
R                         MEDICINE    0.292<br />
M         MUSIC AND BOOKS ON MUSIC    0.229<br />
S                      AGRICULTURE    0.224<br />
H                  SOCIAL SCIENCES    0.215<br />
U                 MILITARY SCIENCE    0.214<br />
B PHILOSOPHY. PSYCHOLOGY. RELIGION    0.211<br />
E          HISTORY OF THE AMERICAS    0.204<br />
Z   BIBLIOGRAPHY. LIBRARY SCIENCE.    0.188<br />
C    AUXILIARY SCIENCES OF HISTORY    0.184</p>
<p>I’m pleased about this, since I’m arguing a lot for the importance of existing educational discourses of attention shaping popular psychology in the 1920s and 30s. BF (Psychology) clocks in at .610. I need to look at the usage, but the prominence of music is good for that half-chapter I’ve been pitching on listening manuals. Mention doesn’t imply contestation, of course—I think the medical category is high because of diagnosis, and military may be just because of the drill word—but it’s not a bad proxy, as they go.</p>
<p>How do we bring the historical dimension back into all this? The numbers are very small for most of these categories, so I think it’s best just to dump some of them for now, and for others use a representation that reminds us just how little data we’re dealing with. If I keep categories with more than 50 or so books, we can get a balloon chart like so: (These aren’t decades, but rounded years–so 1876 and 1884 are with 1880, and 1885 and 1891 with 1890. I don’t like decades for the most part because I think they lead to stereotyping.)</p>
<p><a href="http://4.bp.blogspot.com/_Pge31alC_E8/TRgtkbmqw8I/AAAAAAAACYk/LGKICXkp8bY/s1600/Evolution+Usage+Rates.png"><img src="http://4.bp.blogspot.com/_Pge31alC_E8/TRgtkbmqw8I/AAAAAAAACYk/LGKICXkp8bY/s1600/Evolution+Usage+Rates.png" /></a></p>
<p>But that’s not perfect: it’s good for communicating scale and the vagueness of frequency data, but it’s not really appropriate because there aren’t many words that will have historically interesting info in more than a few cells of the grid. The comparison with “Darwin” is interesting, because it highlights how the discourse of evolution spread outside the science more than it did Darwin’s name:</p>
<p><a href="http://2.bp.blogspot.com/_Pge31alC_E8/TRgvx36CccI/AAAAAAAACYo/_fFNT5gF_l8/s1600/Darwin+Usage+Rates.png"><img src="http://2.bp.blogspot.com/_Pge31alC_E8/TRgvx36CccI/AAAAAAAACYo/_fFNT5gF_l8/s1600/Darwin+Usage+Rates.png" /></a></p>
<p>But that’s a lot of ink to expend on a table that doesn’t even make the numbers legible. I hate side-by-side bar charts, but using width to indicate number might not be bad here. I’ll have to think on it some more. I could see these being useful in some cases where you wanted to know just where a word was being used that had a flattish trend overall, and initially, they’re not bad for reminding what the distribution of books is. (It’s interesting to see that the history classes hardly increase at all from the 1860s. The social science and education classes (H and L) just get bigger and bigger, as do those novels in P.)</p>
<p>Also, since I haven’t said anything about source libraries for the google-scanned books: Why don’t I have call numbers for more than 30% of my books? Does the shortfall have to do with the libraries they come from? Here’s the percentage of my books that have call numbers, by the library IA thinks they came from (minus a few small ones):</p>
<table style="width:75%;">
<colgroup>
<col style="width: 50%" />
<col style="width: 11%" />
<col style="width: 13%" />
</colgroup>
<tbody>
<tr class="odd">
<td>Library                          </td>
<td>Books</td>
<td>Percent</td>
</tr>
<tr class="even">
<td>University of Virginia             University of Wisconsin - Madison Oxford University                 University of California           unknown library                   New York Public Library           University of Michigan             Harvard University                </td>
<td>  138   276   646  3228  4199  6368  6860  9971</td>
<td>23.1884 36.2319 19.1950 34.1078 21.3384 27.2456 30.1603 18.5438</td>
</tr>
</tbody>
</table>
<p>+———————————–+——-+———+</p>
<p>So the Michigan and the Cal books are the best, and they’re some of the biggest partners in Hathi; and Harvard, only just recently a member of Hathi, is the worst. But it’s pretty even across the libraries–using internet archive data, library data alone can’t explain whether there’s call number data. Internet archive is partly to blame–they only have OCLC ids for about two thirds of the books I downloaded, and they don’t nicely integrate them into their downloadable catalogs. But the rest of the variation is odd–there are some gaps between IA’s holdings of books and what I can get.</p>
<h2 id="next-up-is-way-of-integrating-dimensional-analysis-with-catalog-data.-probably-a-little-commentary-before-that-though.">Next up is way of integrating dimensional analysis with catalog data. Probably a little commentary before that, though.</h2>
<h3 id="comments">Comments:</h3>
<h4 id="what-does-agriculture-have-to-do-with-attention">What does agriculture have to do with attention?!</h4>
<p><a href="https://www.blogger.com/profile/13542022273476075921" title="noreply@blogger.com">Jamie</a> - <time datetime="2010-12-28T09:50:15.883-05:00">Dec 2, 2010</time></p>
<p>What does agriculture have to do with attention?!</p>
<hr />
<h4 id="i-think-its-a-use-mention-thingagriculture-b">I think it’s a use-mention thing—agriculture b…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2010-12-30T16:14:13.161-05:00">Dec 4, 2010</time></p>
<p>I think it’s a use-mention thing—agriculture books are probably more instructive than other genres, so they do things like tell farmers to pay careful attention to signs of boll weevil infestation. That’s why, I suspect, medicine is so high as well. They’re not describing attentional disorders in the 19th century, they’re telling doctors to pay attention to symptoms in diagnosing other diseases. Or at least that’s what I’d guess, I still have to make my sentence sampler work on particular genre subsets.</p>
<hr />
<h4 id="i-think-its-slightly-more-interesting-than-th">I think it’s slightly more interesting than th…</h4>
<p><a href="https://www.blogger.com/profile/04856020368342677253">Ben</a> - <time datetime="2010-12-30T20:40:02.800-05:00">Dec 4, 2010</time></p>
<p>I think it’s slightly more interesting than that, after looking at the texts: “Agriculture” also includes a _lot_ of books about hunting and fishing, and animals and fish are frequently described as having attention gained or diverted. Not all of it is like that, but maybe enough to explain the excess over normal. I like that a lot: I’m mostly interested in folk child psychology, but animal psychology adds an interesting dimension I’ve only thought about a little.</p>
<hr />
]]></content>
        <author>
            <name>Ben Schmidt</name>
            <email>bmschmidt@gmail.com</email>
            <uri>https://benschmidt.org</uri>
        </author>
    </entry>
</feed>